[{"id":0,"href":"/doc/main/","title":"API Reference","parent":"g1raffi.doc","content":" Introduction "},{"id":1,"href":"/doc/quarkus/psql/psql/","title":"Reactive","parent":"Quarkus","content":" PSQL Quarkus Reactive psql client ./mvnw quarkus:add-extension -Dextensions=\u0026#34;reactive-pg-client\u0026#34; Starts automatically new devservice psql container with db/user/password = quarkus.\nroot@ac1e2f26032a:/# psql -U quarkus -d quarkus -h localhost -p 5432 Create InitDB Skript like @ApplicationScoped public class DBInit { private final PgPool client; private final boolean schemaCreate; private static final Logger log = Logger.getLogger(DBInit.class.getName()); public DBInit(PgPool client, @ConfigProperty(name = \u0026#34;myapp.schema.create\u0026#34;, defaultValue = \u0026#34;true\u0026#34;) boolean schemaCreate) { this.client = client; this.schemaCreate = schemaCreate; } void onStart(@Observes StartupEvent ev) { if (schemaCreate) { log.info(\u0026#34;Initializing Database\u0026#34;); initDb(); } } private void initDb() { client.query(\u0026#34;DROP TABLE IF EXISTS sensormeasurements\u0026#34;).execute() .flatMap(r -\u0026gt; client.query(\u0026#34;CREATE TABLE sensormeasurements (id SERIAL PRIMARY KEY, data DOUBLE PRECISION, time TIMESTAMP WITH TIME ZONE DEFAULT NOW()::timestamp)\u0026#34;).execute()) .flatMap(r -\u0026gt; client.query(\u0026#34;INSERT INTO sensormeasurements (data) VALUES (0.1)\u0026#34;).execute()) .flatMap(r -\u0026gt; client.query(\u0026#34;INSERT INTO sensormeasurements (data) VALUES (0.2)\u0026#34;).execute()) .flatMap(r -\u0026gt; client.query(\u0026#34;INSERT INTO sensormeasurements (data) VALUES (0.3)\u0026#34;).execute()) .flatMap(r -\u0026gt; client.query(\u0026#34;INSERT INTO sensormeasurements (data) VALUES (0.4)\u0026#34;).execute()) .await().indefinitely(); } } CRUD Operations example import io.smallrye.mutiny.Multi; import io.smallrye.mutiny.Uni; import io.vertx.mutiny.pgclient.PgPool; import io.vertx.mutiny.sqlclient.Row; import io.vertx.mutiny.sqlclient.RowSet; import io.vertx.mutiny.sqlclient.Tuple; import java.time.Instant; import java.time.ZoneOffset; public class SensorMeasurement { public Long id; public Double data; public Instant time; public SensorMeasurement() { this.data = Math.random(); this.time = Instant.now(); } public SensorMeasurement(Row row) { this.id = row.getLong(\u0026#34;id\u0026#34;); this.data = row.getDouble(\u0026#34;data\u0026#34;); this.time = Instant.from(row.getOffsetDateTime(\u0026#34;time\u0026#34;)); } public static Multi\u0026lt;SensorMeasurement\u0026gt; findAll(PgPool client) { return client.query(\u0026#34;SELECT id, data, time from sensormeasurements\u0026#34;).execute() .onItem().transformToMulti(set -\u0026gt; Multi.createFrom().iterable(set)) .onItem().transform(SensorMeasurement::new); } public static Uni\u0026lt;SensorMeasurement\u0026gt; findById(PgPool client, Long id) { return client.preparedQuery(\u0026#34;SELECT id, data, time from sensormeasurements where id = $1\u0026#34;).execute(Tuple.of(id)) .onItem().transform(RowSet::iterator) .onItem().transform(iterator -\u0026gt; iterator.hasNext() ? new SensorMeasurement(iterator.next()) : null); } public static Uni\u0026lt;SensorMeasurement\u0026gt; findLatest(PgPool client) { return client.query(\u0026#34;SELECT id, data, time from sensormeasurements ORDER BY time DESC LIMIT 1\u0026#34;).execute() .map(RowSet::iterator) .map(rowRowIterator -\u0026gt; rowRowIterator.hasNext() ? new SensorMeasurement(rowRowIterator.next()) : null); } public Uni\u0026lt;SensorMeasurement\u0026gt; save(PgPool client) { return client.preparedQuery(\u0026#34;INSERT INTO sensormeasurements (data, time) VALUES ($1, $2) RETURNING (id, data, time)\u0026#34;) .execute(Tuple.of(data, time.atOffset(ZoneOffset.UTC))) .onItem().transform(RowSet::iterator) .onItem().transform(iterator -\u0026gt; iterator.hasNext() ? this : null); } } Generic Map\u0026lt;String, String\u0026gt; jsonb types Add dependencies:\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive-jackson\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-flyway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-hibernate-orm-panache\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkiverse.hibernatetypes\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-hibernate-types\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-jdbc-postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Create migration skripts \u0026ldquo;/resources/db/migration\u0026rdquo;:\nV1.0.0__Model.sql:\nCREATE TABLE public.example ( id integer NOT NULL, content jsonb, CONSTRAINT id_pkey PRIMARY KEY (id) ) V1.0.1__Data.sql:\nINSERT INTO public.example (id, content) VALUES (1, \u0026#39;{ \u0026#34;field\u0026#34;: \u0026#34;value\u0026#34; }\u0026#39;); INSERT INTO public.example (id, content) VALUES (2, \u0026#39;{ \u0026#34;anotherField\u0026#34;: \u0026#34;anotherValue\u0026#34; }\u0026#39;); Create and define entity:\nimport io.quarkiverse.hibernate.types.json.JsonBinaryType; import io.quarkiverse.hibernate.types.json.JsonTypes; import io.quarkus.hibernate.orm.panache.PanacheEntityBase; import org.hibernate.annotations.Type; import org.hibernate.annotations.TypeDef; import org.hibernate.annotations.TypeDefs; import javax.persistence.Entity; import javax.persistence.Id; import java.util.Map; @Entity @TypeDefs({ @TypeDef(name = \u0026#34;jsonb\u0026#34;, typeClass = JsonBinaryType.class) }) public class Example extends PanacheEntityBase { @Id public int id; @Type(type = JsonTypes.JSON_BIN) public Map\u0026lt;String, String\u0026gt; content; } Enjoy!\n"},{"id":2,"href":"/doc/cloud-events/cloud-events/","title":"CloudEvents","parent":"Cloud-events","content":" CloudEvents Github\nCNCF Incubating project since 2018.\nSummary Standardization is a general need in all fields after some technique is widely used. In the last few years the trend for scalable, event-driven systems grew massively. Everybody and everything is communicating in events. And all systems face the same question at some point - what should our events look like. If you were smart enough you asked the question rather earlier than later. If not, at some point you will face the truth that you will have to refactor a lot to achieve consistency in your events throughout your distributed system. This is where you would have wished to know CloudEvents already.\nCloudEvents brings a specification to describe events in a common way. The common language increases consistency, accessibility and portability in distributed systems. Major programming languages like Java, Go, JavaScript, Ruby, Rust, Python have SDKs and APIs to implement CloudEvents in a simple way. At it\u0026rsquo;s core it will bring us a blueprint or language to define a set of metadata to describe the event.\nExample CloudEvent\n{ \u0026#34;specversion\u0026#34; : \u0026#34;1.0\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;com.github.pull_request.opened\u0026#34;, \u0026#34;source\u0026#34; : \u0026#34;https://github.com/cloudevents/spec/pull\u0026#34;, \u0026#34;subject\u0026#34; : \u0026#34;123\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;A234-1234-1234\u0026#34;, \u0026#34;time\u0026#34; : \u0026#34;2018-04-05T17:31:00Z\u0026#34;, \u0026#34;comexampleextension1\u0026#34; : \u0026#34;value\u0026#34;, \u0026#34;comexampleothervalue\u0026#34; : 5, \u0026#34;datacontenttype\u0026#34; : \u0026#34;text/xml\u0026#34;, \u0026#34;data\u0026#34; : \u0026#34;\u0026lt;much wow=\\\u0026#34;xml\\\u0026#34;/\u0026gt;\u0026#34; } Specification It all comes down to a handful attributes:\nRequired attributes:\nattribute type description id String Identifies the event (e.g. UUID) source URI-reference Identifies the context in which an event happened specversion String Version of the CloudEvents specification which the event uses type String Describes the type of event related to the originating occurrence Optional attributes:\nattribute type description datacontenttype String RFC 2046 Content type of data value subject String This describes the subject of the event in the context of the event producer (identified by source). In publish-subscribe scenarios, a subscriber will typically subscribe to events emitted by a source time Timestamp RFC 3339 Timestamp of when the occurrence happened In addition to the specification of CloudEvents itself, there are extensions giving extra flexibility for other meta fields to enrich your event. For example OpenTracing header fields can be added by the Distributed Tracing extension.\nImplementation Let\u0026rsquo;s try to get our hands dirty and test the CloudEvent specification to fire some events. We are going to use the Quarkus framework with the Smallrye reactive messaging extension.\nAs it just happens the Smallrye reactive messaging extension supports CloudEvents out of the box!\nWe create two new Quarkus projects:\nCreation of Quarkus projects\nmvn io.quarkus.platform:quarkus-maven-plugin:2.10.3.Final:create \\ -DprojectGroupId=org.acme \\ -DprojectArtifactId=data-producer \\ -Dextensions=\u0026#34;resteasy-reactive\u0026#34; mvn io.quarkus.platform:quarkus-maven-plugin:2.10.3.Final:create \\ -DprojectGroupId=org.acme \\ -DprojectArtifactId=data-consumer \\ -Dextensions=\u0026#34;resteasy-reactive\u0026#34; Remove the test classes and add the following extensions to your projects\u0026rsquo; pom.xml:\nDependencies in pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-smallrye-reactive-messaging-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-confluent-registry-avro\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.confluent\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kafka-avro-serializer\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;6.1.1\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;jakarta.ws.rs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jakarta.ws.rs-api\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;confluent\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://packages.confluent.io/maven/\u0026lt;/url\u0026gt; \u0026lt;snapshots\u0026gt; \u0026lt;enabled\u0026gt;false\u0026lt;/enabled\u0026gt; \u0026lt;/snapshots\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; For demonstration purposes we use a Avro-schema, which is be the most common approach to manage schemas with Kafka.\nThe Producer Create a Avro schema src/main/avro/SensorMeasurement.avsc with the following content:\n{ \u0026#34;namespace\u0026#34;: \u0026#34;org.acme\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;SensorMeasurement\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } ] } This represents a simple Java POJO to hold some information about measurements we want to emit.\nWe create a service org.acme.KafkaProducer which allows us to emit SensorMeasurements to a defined Channel measurements which we will connect to a Kafka Topic. This could look like the following:\npackage org.acme; import org.eclipse.microprofile.reactive.messaging.Channel; import org.eclipse.microprofile.reactive.messaging.Emitter; import org.eclipse.microprofile.reactive.messaging.Message; import javax.enterprise.context.ApplicationScoped; import javax.inject.Inject; @ApplicationScoped public class KafkaProducer { @Channel(\u0026#34;measurements\u0026#34;) @Inject Emitter\u0026lt;SensorMeasurement\u0026gt; sensorMeasurementEmitter; public void emitEvent(SensorMeasurement sensorMeasurement) { sensorMeasurementEmitter.send(Message.of(sensorMeasurement)); } } Then we alter the pre-generated REST resource to emit an event every time we receive a POST request to the endpoint /measurements:\npackage org.acme; import javax.ws.rs.POST; import javax.ws.rs.Path; import javax.ws.rs.core.Response; import java.util.Random; @Path(\u0026#34;/measurements\u0026#34;) public class MeasurementsResource { private final KafkaProducer kafkaProducer; public MeasurementsResource(KafkaProducer kafkaProducer) { this.kafkaProducer = kafkaProducer; } @POST public Response emitMeasurement() { SensorMeasurement measurement = SensorMeasurement.newBuilder().setData(new Random().nextDouble()).build(); kafkaProducer.emitEvent(measurement); return Response.ok().build(); } } Of course we need some configuration in the application.properties to emit the events to our Kafka broker:\nmp.messaging.outgoing.measurements.connector=smallrye-kafka mp.messaging.outgoing.measurements.value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer mp.messaging.outgoing.measurements.topic=measurements mp.messaging.outgoing.measurements.cloud-events-source=event-producer mp.messaging.outgoing.measurements.cloud-events-type=measurement-emitted mp.messaging.outgoing.measurements.cloud-events-subject=subject-123 And this is where the magic happens: Configuring the channel \u0026ldquo;measurements\u0026rdquo; with additional properties cloud-events-XXX will simply enrich the message envelope with the properties defined. If you don\u0026rsquo;t like the config approach and would rather enrich the message programmatically, I got you covered!\npublic void emitEvent(SensorMeasurement sensorMeasurement) { OutgoingCloudEventMetadata\u0026lt;Object\u0026gt; metadata = OutgoingCloudEventMetadata.builder() .withId(UUID.randomUUID().toString()) .withSource(URI.create(\u0026#34;event-producer\u0026#34;)) .withType(\u0026#34;measurement-emitted\u0026#34;) .withSubject(\u0026#34;subject-123\u0026#34;) .build(); sensorMeasurementEmitter.send(Message.of(sensorMeasurement).addMetadata(metadata)); } And that\u0026rsquo;s all you need to create events to our little system!\nThe consumer The data-consumer\u0026rsquo;s side of the system looks quite similar. Create a Avro schema src/main/avro/SensorMeasurement.avsc with the following content:\n{ \u0026#34;namespace\u0026#34;: \u0026#34;org.acme\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;SensorMeasurement\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } ] } Instead of producing messages, we will simply define a listener on a channel connected to the same Kafka topic and print the events to the command line!\nCreate the EventListener org.acme.EventListener:\npackage org.acme; import io.smallrye.reactive.messaging.ce.IncomingCloudEventMetadata; import org.eclipse.microprofile.reactive.messaging.Incoming; import org.eclipse.microprofile.reactive.messaging.Message; import org.jboss.logging.Logger; import javax.enterprise.context.ApplicationScoped; import java.util.concurrent.CompletionStage; @ApplicationScoped public class EventListener { private final Logger logger = Logger.getLogger(EventListener.class); @Incoming(\u0026#34;measurements\u0026#34;) public CompletionStage\u0026lt;Void\u0026gt; consume(Message\u0026lt;SensorMeasurement\u0026gt; message) { IncomingCloudEventMetadata cloudEventMetadata = message.getMetadata(IncomingCloudEventMetadata.class).orElseThrow(() -\u0026gt; new IllegalArgumentException(\u0026#34;Expected a CloudEvent!\u0026#34;)); logger.infof(\u0026#34;Received Cloud Events (spec-version: %s): id: \u0026#39;%s\u0026#39;, source: \u0026#39;%s\u0026#39;, type: \u0026#39;%s\u0026#39;, subject: \u0026#39;%s\u0026#39;, payload-message: \u0026#39;%s\u0026#39; \u0026#34;, cloudEventMetadata.getSpecVersion(), cloudEventMetadata.getId(), cloudEventMetadata.getSource(), cloudEventMetadata.getType(), cloudEventMetadata.getSubject().orElse(\u0026#34;no subject\u0026#34;), message.getPayload()); return message.ack(); } } Add the following properties to the application.properties file:\nmp.messaging.incoming.measurements.connector=smallrye-kafka mp.messaging.incoming.measurements.topic=measurements quarkus.http.port=8081 And that\u0026rsquo;s all you need to have your application up and running! If you have Docker installed, starting the application will also start a mock Kafka broker and connect your applications automatically. If you don\u0026rsquo;t have Docker installed you will need to configure the connection to a Kafka broker yourself by adding the following property to the applications:\nkafka.bootstrap.servers=your-kafka-broker:9092 Test your events Start both of your applications in your favorite IDE or shell:\n./mvnw compile quarkus:dev Fire some requests against your producer and test your CloudEvents getting emitted and consumed!\n$ curl -X POST localhost:8080/measurements 2022-07-21 11:01:56,654 INFO [org.acm.EventListener] (vert.x-eventloop-thread-10) Received Cloud Events (spec-version: 1.0): id: \u0026#39;98d85610-6d8d-4943-b8ea-641c4940e148\u0026#39;, source: \u0026#39;event-producer\u0026#39;, type: \u0026#39;measurement-emitted\u0026#39;, subject: \u0026#39;subject-123\u0026#39;, payload-message: \u0026#39;{\u0026#34;data\u0026#34;: 0.12169099891061863}\u0026#39; Recap With the CloudEvents standard you can increase consistency, accessibility and portability of your microservice architecture. You can save time discussing what events should look like and increase efficiency by simply implementing your events and get stuff done!\n"},{"id":3,"href":"/doc/quarkus/junit/junit/","title":"JUnit","parent":"Quarkus","content":" Creating Dynamic Tests with JUnit 5 Event.java:\npackage io.quarkus.mcve.entity; import io.quarkiverse.hibernate.types.json.JsonBinaryType; import io.quarkiverse.hibernate.types.json.JsonTypes; import io.quarkus.hibernate.orm.panache.PanacheEntityBase; import org.hibernate.annotations.Type; import org.hibernate.annotations.TypeDef; import org.hibernate.annotations.TypeDefs; import javax.persistence.Entity; import javax.persistence.Id; import java.time.Instant; import java.util.Map; import java.util.UUID; @Entity @TypeDefs({ @TypeDef(name = \u0026#34;jsonb\u0026#34;, typeClass = JsonBinaryType.class) }) public class Event extends PanacheEntityBase { @Id public UUID id; public String description; public Instant timestamp; @Type(type = JsonTypes.JSON_BIN) public Map\u0026lt;String, String\u0026gt; details; @Type(type = JsonTypes.JSON_BIN) public Map\u0026lt;String, String\u0026gt; monitoring; } Configuration yaml file:\n/resources/application.yaml:\nquarkus: flyway: migrate-at-start: true hibernate-orm: database.generation: none testconfig: suites: - name: suite1 api: localhost:8080 request-body: \u0026#39;{\u0026#34;user\u0026#34;: \u0026#34;g1raffi\u0026#34;}\u0026#39; correlation-id: 1 assertions: - name: assertion1 from-state: FIRST_STATE to-state: SECOND_STATE max-time-ms: 3100 - name: assertion2 from-state: FIRST_STATE to-state: THIRD_STATE max-time-ms: 4100 TestConfiguration.java:\nThe test configuration files\npackage io.quarkus.mcve.config; import io.smallrye.config.ConfigMapping; import java.util.List; @ConfigMapping(prefix = \u0026#34;testconfig\u0026#34;) public interface TestConfiguration { List\u0026lt;TestSuite\u0026gt; suites(); } TestSuite.java:\npackage io.quarkus.mcve.config; import java.util.List; public interface TestSuite { String name(); String correlationId(); String api(); String requestBody(); List\u0026lt;TestAssertion\u0026gt; assertions(); } TestAssertion.java:\npackage io.quarkus.mcve.config; public interface TestAssertion { String name(); String fromState(); String toState(); Double maxTimeMs(); } DB Migration with Flyway:\n/resources/db/migration/V1.0.0__Data.sql:\nCREATE TABLE public.event ( id uuid NOT NULL, description text, timestamp timestamp, details jsonb, monitoring jsonb, CONSTRAINT id_pkey PRIMARY KEY (id) ); INSERT INTO public.event (id, description, \u0026#34;timestamp\u0026#34;, details, monitoring) VALUES (\u0026#39;11111111-1111-1111-1111-111111111111\u0026#39;, \u0026#39;description1\u0026#39;, \u0026#39;2022-09-06T12:08:42Z\u0026#39;, \u0026#39;{ \u0026#34;state\u0026#34;: \u0026#34;FIRST_STATE\u0026#34;, \u0026#34;correlation-id\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39;, \u0026#39;{}\u0026#39;); INSERT INTO public.event (id, description, \u0026#34;timestamp\u0026#34;, details, monitoring) VALUES (\u0026#39;44444444-4444-4444-4444-444444444444\u0026#39;, \u0026#39;description4\u0026#39;, \u0026#39;2022-09-06T12:08:45Z\u0026#39;, \u0026#39;{ \u0026#34;state\u0026#34;: \u0026#34;SECOND_STATE\u0026#34;, \u0026#34;correlation-id\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39;, \u0026#39;{}\u0026#39;); INSERT INTO public.event (id, description, \u0026#34;timestamp\u0026#34;, details, monitoring) VALUES (\u0026#39;55555555-5555-5555-5555-555555555555\u0026#39;, \u0026#39;description5\u0026#39;, \u0026#39;2022-09-06T12:08:46Z\u0026#39;, \u0026#39;{ \u0026#34;state\u0026#34;: \u0026#34;THIRD_STATE\u0026#34;, \u0026#34;correlation-id\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39;, \u0026#39;{}\u0026#39;); Finally the test class\ntest/java/\u0026hellip;/DynamicTestSuites.java:\npackage io.quarkus.mcve; import io.quarkus.mcve.config.TestAssertion; import io.quarkus.mcve.config.TestConfiguration; import io.quarkus.mcve.entity.Event; import io.quarkus.test.junit.QuarkusTest; import org.junit.jupiter.api.Assertions; import org.junit.jupiter.api.DynamicTest; import org.junit.jupiter.api.TestFactory; import javax.inject.Inject; import java.util.List; import java.util.Map; import java.util.function.Function; import java.util.stream.Collectors; import java.util.stream.Stream; import static io.restassured.RestAssured.given; import static org.hamcrest.CoreMatchers.is; @QuarkusTest public class DynamicTestSuites { @Inject TestConfiguration testConfig; @TestFactory public Stream\u0026lt;DynamicTest\u0026gt; testSuiteSize() { return testConfig.suites().stream() .map(testSuite -\u0026gt; DynamicTest.dynamicTest( \u0026#34;Running suite: \u0026#34; + testSuite.name(), () -\u0026gt; { // Fire Event makeRestCall(testSuite.api(), testSuite.requestBody()); // Sleep for Events to finish // Get Event Chain List\u0026lt;Event\u0026gt; eventChain = getEventChain(); // Make assertions for (TestAssertion assertion : testSuite.assertions()) { System.out.println(\u0026#34;maxTime: \u0026#34; + assertion.maxTimeMs()); System.out.println(\u0026#34;getTimeBetweenEvents: \u0026#34; + getTimeBetweenEvents(eventChain, assertion.fromState(), assertion.toState())); Assertions.assertTrue(assertion.maxTimeMs() \u0026gt; getTimeBetweenEvents(eventChain, assertion.fromState(), assertion.toState())); } } )); } private long getTimeBetweenEvents(List\u0026lt;Event\u0026gt; eventChain, String fromState, String toState) { Map\u0026lt;String, Event\u0026gt; stateMap = eventChain.stream().collect(Collectors.toMap( event -\u0026gt; event.details.get(\u0026#34;state\u0026#34;), Function.identity() )); System.out.println(\u0026#34;getTimeBetweenEvents called\u0026#34;); System.out.println(\u0026#34;fromState time: \u0026#34; + stateMap.get(fromState).timestamp.toEpochMilli()); System.out.println(\u0026#34;toState time: \u0026#34; + stateMap.get(toState).timestamp.toEpochMilli()); return stateMap.get(toState).timestamp.toEpochMilli() - stateMap.get(fromState).timestamp.toEpochMilli(); } private List\u0026lt;Event\u0026gt; getEventChain() { return Event.listAll(); } private void makeRestCall(String api, String requestBody) { } } \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive-jackson\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-flyway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-hibernate-orm-panache\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkiverse.hibernatetypes\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-hibernate-types\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-config-yaml\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-jdbc-postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-junit5\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.rest-assured\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rest-assured\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Tada!\n"},{"id":4,"href":"/doc/quarkus/reactive/","title":"Reactive","parent":"Quarkus","content":" Reactive World Emit periodic messages SomeEmitter.class\n@ConfigProperty(name = \u0026#34;application.polling.interval-seconds\u0026#34;, defaultValue = \u0026#34;900\u0026#34;) int pollingInterval; @Outgoing(\u0026#34;Trigger\u0026#34;) public Flowable\u0026lt;String\u0026gt; triggerUpdateEvent() { return Flowable.interval(pollingInterval, TimeUnit.SECONDS) .map(tick -\u0026gt; \u0026#34;triggered\u0026#34;); } @Incoming(\u0026#34;Trigger\u0026#34;) @Outgoing(\u0026#34;InboundReading\u0026#34;) public PublisherBuilder\u0026lt;List\u0026lt;E\u0026gt;\u0026gt; update(String triggered) { log.debug(\u0026#34;trigger received\u0026#34;); E e = result(); return ReactiveStreams.of(e); } Add dependencies pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-reactive-pg-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-flyway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-jdbc-postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive-jsonb\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Add dependencies to your code:\nReactive PgClient Create a @ApplicationScoped repository for the data access:\nimport io.smallrye.mutiny.Multi; import io.vertx.mutiny.pgclient.PgPool; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class DataRepository { private final PgPool pgPool; public DataRepository(PgPool pgPool) { this.pgPool = pgPool; } Multi\u0026lt;Data\u0026gt; findAllData() { return pgPool.query(\u0026#34;SELECT * FROM jsondata\u0026#34;) .execute() .onItem() .transformToMulti(i -\u0026gt; Multi.createFrom().iterable(i)) .map(i -\u0026gt; new Data(i.getUUID(\u0026#34;id\u0026#34;).toString(), i.getString(\u0026#34;json\u0026#34;))); } } Migration Create flyway migration script to initialize table and add dummy data:\nsrc/main/resources/db/migration/V1.0.0__Quarkus.sql\nCREATE TABLE IF NOT EXISTS jsondata ( id uuid PRIMARY KEY, json text ); INSERT INTO jsondata (id, json) VALUES (\u0026#39;21c23437-89d1-4774-bbbe-c286fb7c3afd\u0026#39;, \u0026#39;teststring\u0026#39;); Configuration Add configuration to enable auto migration:\napplication.property\nquarkus.flyway.migrate-at-start=true Rest Resource Alter the REST resource to return the object from the database:\nDataResource.java\npackage org.acme; import io.smallrye.mutiny.Multi; import javax.inject.Inject; import javax.ws.rs.GET; import javax.ws.rs.Path; import javax.ws.rs.Produces; import javax.ws.rs.core.MediaType; @Path(\u0026#34;/data\u0026#34;) public class DataResource { @Inject DataRepository dataRepository; @GET @Produces(MediaType.APPLICATION_JSON) public Multi\u0026lt;Data\u0026gt; hello() { return dataRepository.findAllData(); } } Data.java\npackage org.acme; public class Data { public String id; public String data; public Data() { } public Data(String id, String data) { this.id = id; this.data = data; } } "},{"id":5,"href":"/doc/linux/fedora/","title":"Fedora","parent":"Linux","content":" Switch to fedora Docker replaced with podman:\nNon-root problematic with podman and all the other docker-like resources are fixed by setting:\n# Install the required podman packages from dnf. If you\u0026#39;re not using rpm based # distro, replace with respective package manager sudo dnf install podman podman-docker # Enable the podman socket with Docker REST API systemctl --user enable podman.socket --now # Set the required envvars export DOCKER_HOST=unix:///run/user/${UID}/podman/podman.sock export TESTCONTAINERS_RYUK_DISABLED=true "},{"id":6,"href":"/doc/kubernetes/postgres/","title":"Postgres","parent":"Kubernetes","content":"To deploy and start ephemereal postgres container apply the following manifest:\n--- apiVersion: v1 kind: Service metadata: name: postgres labels: app: postgres spec: selector: app: postgres ports: - protocol: TCP port: 5432 targetPort: 5432 --- apiVersion: apps/v1 kind: Deployment metadata: name: postgres labels: app: postgres spec: replicas: 1 selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: containers: - name: postgres image: postgres:latest ports: - containerPort: 5432 env: - name: POSTGRES_PASSWORD value: password - name: POSTGRES_USER value: user - name: POSTGRES_DB value: db - name: PGDATA value: /var/lib/postgresql/data/pgdata volumeMounts: - mountPath: /var/lib/postgresql/data name: psqldata readOnly: false volumes: - name: psqldata emptyDir: {} "},{"id":7,"href":"/doc/argocd/","title":"Argocd","parent":"g1raffi.doc","content":" ArgoCD Create application argocd apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: pitc-rhe-serverless namespace: pitc-infra-argocd spec: destination: namespace: ${NAMESPACE} server: # Clusterroute or Clustername e.g.: https://api.ocp-staging.cloudscale.puzzle.ch:6443 project: pitc-apps source: path: src/main/openshift repoURL: https://${TOKEN_NAME}:${TOKEN_VALUE}@gitlab.puzzle.ch/some-repo.git - CreateNamespace=true\n"},{"id":8,"href":"/doc/eda/","title":"Eda","parent":"g1raffi.doc","content":" event driven architecture https://www.youtube.com/watch?v=uxPFoImWpBg\u0026t=7200s\n"},{"id":9,"href":"/doc/kubernetes/hacky-windoof/","title":"Hacky Windoof","parent":"Kubernetes","content":" Windoof CP File from pod to windows in gitbash\noc exec PODNAME -c CONTAINER_NAME -- bash -c \u0026#34;base64 FILE\u0026#34; | base64 -d \u0026gt; localfile "},{"id":10,"href":"/doc/kubernetes/helm/helm/","title":"Helm","parent":"Kubernetes","content":" Helm Post install hook to test CRD readiness apiVersion: batch/v1 kind: Job metadata: name: \u0026#34;{{ .Release.Name }}\u0026#34; labels: app.kubernetes.io/managed-by: {{ .Release.Service | quote }} app.kubernetes.io/instance: {{ .Release.Name | quote }} app.kubernetes.io/version: {{ .Chart.AppVersion }} helm.sh/chart: \u0026#34;{{ .Chart.Name }}-{{ .Chart.Version }}\u0026#34; annotations: # This is what defines this resource as a hook. Without this line, the # job is considered part of the release. \u0026#34;helm.sh/hook\u0026#34;: post-install \u0026#34;helm.sh/hook-weight\u0026#34;: \u0026#34;-5\u0026#34; \u0026#34;helm.sh/hook-delete-policy\u0026#34;: hook-succeeded spec: template: metadata: name: \u0026#34;{{ .Release.Name }}\u0026#34; labels: app.kubernetes.io/managed-by: {{ .Release.Service | quote }} app.kubernetes.io/instance: {{ .Release.Name | quote }} helm.sh/chart: \u0026#34;{{ .Chart.Name }}-{{ .Chart.Version }}\u0026#34; spec: restartPolicy: Never containers: {{- range .Values.operators }} - name: post-install-{{ .name }} image: \u0026#34;openshift4/ose-cli\u0026#34; command: [\u0026#34;test=$(oc get {{ .crd }} | grep NAME \u0026amp;\u0026gt; /dev/null); while [ $? -ne 0 ]; do sleep 5; echo \u0026#34;waiting\u0026#34;; test=$(oc get {{ .crd }} | grep NAME \u0026amp;\u0026gt; /dev/null); done;\u0026#34;] {{- end }} "},{"id":11,"href":"/doc/kubernetes/istio/servicemesh/","title":"Servicemesh","parent":"Kubernetes","content":" Service Mesh Operator (Istio) Installing Operator Install ElasticSearch Operator Install Jaeger Operator Install Service Mesh Operator Install Kiali Operator Setup the control plane UI Method:\nCreate new ServiceMeshControlPlane:\nCreate namespace (e.g. istio-system) Openshift Web Console Installed Operators Openshift Service Mesh Istio ServiceMeshControlPlane Create new Control Plane Declarative Method:\nCreate file for ServiceMeshControlPlane (smcp):\napiVersion: maistra.io/v2 kind: ServiceMeshControlPlane metadata: finalizers: - maistra.io/istio-operator generation: 1 name: basic namespace: istio-system spec: grafana: enabled: true jaeger: install: storage: type: Memory kiali: enabled: true prometheus: enabled: true policy: type: Istiod profiles: - default telemetry: type: Istiod tracing: sampling: 10000 type: Jaeger version: v2.0 Create the ServiceMeshMemberRoll Create ServiceMeshMemberRoll (smmr):\nOnly projects registered per ServiceMeshMemberRoll will get included in the contorl plane. There must be a ServiceMeshMemberRoll named default in the project of the ServiceMeshControlPlane.\napiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default namespace: istio-system spec: members: # a list of projects joined into the service mesh # - your-project-name # - another-project-name Update and add the projects to the ServiceMeshMemberRoll and apply the resource. Opt in deployments into control plane Workload must opt-in to be tracked by the control plane. This happens with the annotation: \u0026ldquo;sidecar.istio.io/inject\u0026rdquo;:\u0026ldquo;true\u0026rdquo;. Patch the desired resources to add the annotation to the workload that should be tracked.\noc patch deployment DEPLOYMENTNAME -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;sidecar.istio.io/inject\u0026#34;:\u0026#34;true\u0026#34;}}}}}\u0026#39; --type=merge To remove services from the service mesh delete the namespace from the ServiceMeshMemberRoll:\n$ oc -n istio-system patch --type=\u0026#39;json\u0026#39; smmr default -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/members\u0026#34;, \u0026#34;value\u0026#34;:[\u0026#34;\u0026#39;\u0026#34;NAMESPACE_TO_REMOVE\u0026#34;\u0026#39;\u0026#34;]}]\u0026#39; To add services to the service mesh add the namespace to the ServiceMeshMemberRoll:\noc -n istio-system patch --type=\u0026#39;json\u0026#39; smmr default -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/members\u0026#34;, \u0026#34;value\u0026#34;:[\u0026#34;\u0026#39;\u0026#34;NAMESPACE_TO_ADD\u0026#34;\u0026#39;\u0026#34;]}]\u0026#39; Getting applications to run in a service mesh Add the application to the ServiceMeshMemberRoll:\noc -n istio-system patch --type=\u0026#39;json\u0026#39; smmr default -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/members\u0026#34;, \u0026#34;value\u0026#34;:[\u0026#34;\u0026#39;\u0026#34;NAMESPACE_TO_ADD\u0026#34;\u0026#39;\u0026#34;]}]\u0026#39; Patch your Workload to opt-in with sidecar enrollment:\noc patch deployment DEPLOYMENTNAME -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;sidecar.istio.io/inject\u0026#34;:\u0026#34;true\u0026#34;}}}}}\u0026#39; --type=merge "},{"id":12,"href":"/doc/kubernetes/openshift/amq/amq/","title":"Amq","parent":"Kubernetes","content":" AMQ Openshift Setup Links Blog Red Hat\nInstall Operator\nCheck and install resources.\nConfiguration Almost all aspects of the broker on openshift are configured via the CR available. Most settings can be applied in the main resource ActiveMQArtemis.\nAddresses Adresses can be configured with the ActiveMQArtemisAddress custom resource. An example can be found here.\nBroker The broker will be configured with the ActiveMQArtemis custom resouce. Example.\nSecurit** Users, Groups and their roles and access strategies to addresses / queues can be defined in the ActiveMQArtemisSecurity custom resource.\nHA Setup Broker On Kubernetes, broker HA is achieved through health checks and container restarts. On-premise, the broker HA is achieved through master/slave (shared store or replication). When replication is used, the slave will already hold the queues in memory, and therefore is pretty much ready to go in case of failover. With shared storage, when the slave gets hold of the lock, then the queues need to be read from the journals ahead of the slave takeover. The time for a shared storage slave to take over will be dependent on the number and size of messages in the journal.\nWhen we talk about broker HA, it comes down to an active-passive failover mechanism (with Kubernetes being an exception). But Artemis also has an active-active clustering mechanism used primarily for scalability rather than HA. In active-active clustering, every message belongs to only one broker, and losing an active broker will make its messages also unaccessible—but a positive side effect of that issue is that the broker infrastructure is still up and functioning. Clients can use active instances and exchange messages with the drawback of temporarily not accessing the messages that are in the failed broker. To sum up, active-active clustering is primarily for scalability, but it also partially improves the availability with temporary message unavailability. Load balancer\nIf there is a load balancer, prefer one that is already HA in the organization, such as F5s. If Qpid is used, you will need two or more active instances for high availability. Clients\nThis is probably the easiest part, as most customers will already run the client services in redundantly HA fashion, which means two or more instances of consumers and producers most of the time. A side effect of running multiple consumers is that message ordering is not guaranteed. This is where message groups and exclusive consumers can be used.\nScalability Scalability is relatively easier to achieve with Artemis. Primarily, there are two approaches to scaling the message broker. Active-active clustering\nCreate a single logical broker cluster that is scaled transparently from the clients. This can be three masters and three slaves (replication or shared storage doesn’t matter) to start with, which means that clients can use any of the masters to produce and consume the messages. The broker will perform load balancing and message distributions. Such a messaging infrastructure is scalable and supports many queues and topics with different messaging patterns. Artemis can handle large and small messages effectively, so there is no need for using separate broker clusters depending on the message size either.\nA few of the consequences of active-active clustering are:\nMessage ordering is not preserved. Message grouping needs to be clustered. Scaling down requires message draining. Browsing the brokers and the queues is not centralized. Mutual TLS (no istia) Client side When a client tries to connect to a broker Pod in your deployment, the verifyHost option in the client connection URL determines whether the client compares the Common Name (CN) of the broker’s certificate to its host name, to verify that they match. The client performs this verification if you specify verifyHost=true or similar in the client connection URL.\nBroker side Generate a self-signed certificate for the broker key store.\n$ keytool -genkey -alias broker -keyalg RSA -keystore ~/broker.ks\nExport the certificate from the broker key store, so that it can be shared with clients. Export the certificate in the Base64-encoded .pem format. For example:\n$ keytool -export -alias broker -keystore ~/broker.ks -file ~/broker_cert.pem\nOn the client, create a client trust store that imports the broker certificate.\n$ keytool -import -alias broker -keystore ~/client.ts -file ~/broker_cert.pem\nOn the client, generate a self-signed certificate for the client key store.\n$ keytool -genkey -alias broker -keyalg RSA -keystore ~/client.ks\nOn the client, export the certificate from the client key store, so that it can be shared with the broker. Export the certificate in the Base64-encoded .pem format. For example:\n$ keytool -export -alias broker -keystore ~/client.ks -file ~/client_cert.pem\nCreate a broker trust store that imports the client certificate.\n$ keytool -import -alias broker -keystore ~/broker.ts -file ~/client_cert.pem\nLog in to OpenShift Container Platform as an administrator. For example:\n$ oc login -u system:admin\nSwitch to the project that contains your broker deployment. For example:\n$ oc project \u0026lt;my_openshift_project\u0026gt;\nCreate a secret to store the TLS credentials. For example:\n$ oc create secret generic my-tls-secret \\ --from-file=broker.ks=~/broker.ks \\ --from-file=client.ts=~/broker.ts \\ --from-literal=keyStorePassword=\u0026lt;password\u0026gt; \\ --from-literal=trustStorePassword=\u0026lt;password\u0026gt; Note\nWhen generating a secret, OpenShift requires you to specify both a key store and a trust store. The trust store key is generically named client.ts. For two-way TLS between the broker and a client, you must generate a secret that includes the broker trust store, because this holds the client certificate. Therefore, in the preceding step, the value that you specify for the client.ts key is actually the broker trust store file.\nLink the secret to the service account that you created when installing the Operator. For example:\n$ oc secrets link sa/amq-broker-operator secret/my-tls-secret\nSpecify the secret name in the sslSecret parameter of your secured acceptor or connector. For example:\nspec: ... acceptors: - name: my-acceptor protocols: amqp,openwire port: 5672 sslEnabled: true sslSecret: my-tls-secret expose: true connectionsAllowed: 5 ... "},{"id":13,"href":"/doc/kubernetes/openshift/amq/consumer/README/","title":"Readme","parent":"Kubernetes","content":" consumer Project This project uses Quarkus, the Supersonic Subatomic Java Framework.\nIf you want to learn more about Quarkus, please visit its website: https://quarkus.io/ .\nRunning the application in dev mode You can run your application in dev mode that enables live coding using:\n./mvnw compile quarkus:dev NOTE: Quarkus now ships with a Dev UI, which is available in dev mode only at http://localhost:8080/q/dev/.\nPackaging and running the application The application can be packaged using:\n./mvnw package It produces the quarkus-run.jar file in the target/quarkus-app/ directory. Be aware that it’s not an über-jar as the dependencies are copied into the target/quarkus-app/lib/ directory.\nThe application is now runnable using java -jar target/quarkus-app/quarkus-run.jar.\nIf you want to build an über-jar, execute the following command:\n./mvnw package -Dquarkus.package.type=uber-jar The application, packaged as an über-jar, is now runnable using java -jar target/*-runner.jar.\nCreating a native executable You can create a native executable using:\n./mvnw package -Pnative Or, if you don\u0026rsquo;t have GraalVM installed, you can run the native executable build in a container using:\n./mvnw package -Pnative -Dquarkus.native.container-build=true You can then execute your native executable with: ./target/consumer-1.0.0-SNAPSHOT-runner\nIf you want to learn more about building native executables, please consult https://quarkus.io/guides/maven-tooling.\nRelated Guides SmallRye Reactive Messaging - AMQP Connector (guide): Connect to AMQP with Reactive Messaging Provided Code Reactive Messaging codestart Use SmallRye Reactive Messaging\nRelated Apache AMQP 1.0 guide section\u0026hellip;\n"},{"id":14,"href":"/doc/kubernetes/openshift/amq/producer/README/","title":"Readme","parent":"Kubernetes","content":" producer Project This project uses Quarkus, the Supersonic Subatomic Java Framework.\nIf you want to learn more about Quarkus, please visit its website: https://quarkus.io/ .\nRunning the application in dev mode You can run your application in dev mode that enables live coding using:\n./mvnw compile quarkus:dev NOTE: Quarkus now ships with a Dev UI, which is available in dev mode only at http://localhost:8080/q/dev/.\nPackaging and running the application The application can be packaged using:\n./mvnw package It produces the quarkus-run.jar file in the target/quarkus-app/ directory. Be aware that it’s not an über-jar as the dependencies are copied into the target/quarkus-app/lib/ directory.\nThe application is now runnable using java -jar target/quarkus-app/quarkus-run.jar.\nIf you want to build an über-jar, execute the following command:\n./mvnw package -Dquarkus.package.type=uber-jar The application, packaged as an über-jar, is now runnable using java -jar target/*-runner.jar.\nCreating a native executable You can create a native executable using:\n./mvnw package -Pnative Or, if you don\u0026rsquo;t have GraalVM installed, you can run the native executable build in a container using:\n./mvnw package -Pnative -Dquarkus.native.container-build=true You can then execute your native executable with: ./target/producer-1.0.0-SNAPSHOT-runner\nIf you want to learn more about building native executables, please consult https://quarkus.io/guides/maven-tooling.\nRelated Guides SmallRye Reactive Messaging - AMQP Connector (guide): Connect to AMQP with Reactive Messaging Provided Code Reactive Messaging codestart Use SmallRye Reactive Messaging\nRelated Apache AMQP 1.0 guide section\u0026hellip;\n"},{"id":15,"href":"/doc/kubernetes/openshift/crc/","title":"CRC","parent":"Kubernetes","content":" Code Ready Containers Delete existing cluster\n# $ crc delete Setup and start cluster\n# $ crc setup $ crc start "},{"id":16,"href":"/doc/kubernetes/openshift/openshift/","title":"Openshift","parent":"Kubernetes","content":" OpenShift Go Templates Get route host\ncurl $(oc get route ROUTE_NAME -o go-template=\u0026#39;{{(index .status.ingress 0).host}}\u0026#39;)/data Compare ready and desired pod count\n[[ $(oc -n NAMESPACE get dc DC_NAME -o go-template=\u0026#39;{{.status.readyReplicas}}\u0026#39;) == $(oc -n NAMESPACE get dc DC_NAME -o go-template=\u0026#39;{{.status.replicas}}\u0026#39;) ]] Compare environment variables of pods\n[[ $(oc get pod data-consumer-75f69c845d-564bq -o go-template=\u0026#39;{{(index (index .spec.containers 0).env 1)}}\u0026#39;) == $(oc get pod data-producer-74f5d89975-vwxhw -o go-template=\u0026#39;{{(index (index .spec.containers 0).env 0)}}\u0026#39;) ]] Simple automated apply environment script #!/bin/bash echo \u0026#34;You are currently on project:\u0026#34; echo \u0026#34;-------------\u0026#34; oc status | head -1 echo \u0026#34;-------------\u0026#34; echo \u0026#34;Are you sure you want to apply all resources for environment \u0026#39;$1\u0026#39;?\u0026#34; echo \u0026#34;Press \u0026lt;Enter\u0026gt; to do so. Or press \u0026lt;Ctrl\u0026gt;+\u0026lt;C\u0026gt; to abort\u0026#34; read DUMMY ENVIRONMENT=$1 SECRETS=\u0026#34;artemis-credentials db-credentials jwt-tokens postgres-credentials\u0026#34; INFRASTRUCTURE=\u0026#34;routes artemis db-backup waf service-monitor-artemis-activemq service-monitor-postgresql service-monitor-spring-boot\u0026#34; APPS=\u0026#34;application\u0026#34; STATIC=\u0026#34;\u0026#34; DIR=\u0026#34;$(cd \u0026#34;$(dirname \u0026#34;${BASH_SOURCE[0]}\u0026#34; )\u0026#34; \u0026amp;\u0026amp; pwd)\u0026#34; if [ -z \u0026#34;$ENVIRONMENT\u0026#34; ]; then echo \u0026#39;Usage: ./apply-env.sh ENVIRONMENT\u0026#39; exit 1 fi if [ ! -z \u0026#34;$2\u0026#34; ]; then APPS=$2 fi ## configuration script for the environment ## # apply all secrets echo \u0026#39;Applying all secrets and configurations\u0026#39; for secret in ${SECRETS}; do if [ -f \u0026#34;${DIR}/../template/secret-${secret}.yml\u0026#34; ]; then oc get secret ${secret} || oc process -f ${DIR}/../template/secret-${secret}.yml --param-file \\ ${DIR}/../environments/env.yml --ignore-unknown-parameters \\ | oc apply -f - fi done echo \u0026#39;Applying all infrastructure resources\u0026#39; for infra in ${INFRASTRUCTURE}; do if [ -f \u0026#34;${DIR}/../template/infra-${infra}.yml\u0026#34; ]; then oc process -f ${DIR}/../template/infra-${infra}.yml --param-file \\ ${DIR}/../environments/env.yml --ignore-unknown-parameters \\ | oc apply -f - fi done echo \u0026#39;Applying all application resources\u0026#39; for app in ${APPS}; do if [ -f \u0026#34;${DIR}/../template/app-${app}.yml\u0026#34; ]; then oc process -f ${DIR}/../template/app-${app}.yml --param-file \\ ${DIR}/../environments/env.yml --ignore-unknown-parameters \\ | oc apply -f - fi done echo \u0026#39;Applying all static resources\u0026#39; for stat in ${STATIC}; do if [ -f \u0026#34;${DIR}/../static/${stat}.yml\u0026#34; ]; then oc apply -f ${DIR}/../static/${stat}.yml fi done echo \u0026#39;Done!\u0026#39; "},{"id":17,"href":"/doc/kubernetes/prometheus/","title":"Prometheus","parent":"Kubernetes","content":" Prometheus Get CPU and Memory usage of kubernetes pods Get effecitve cpu usage of pod rate(conatiner_cpu_usage_seconds_total{container_name != \u0026#34;\u0026#34;, namespace = \u0026#34;\u0026#34;}[5m]) Cpu requests kube_pod_container_resource_requests_cpu_cores{namespace = \u0026#34;\u0026#34;} Effective memory usage sum_by(pod_name, instance)(container_memory_working_set_bytes{namespace=\u0026#34;\u0026#34;} / 1024^3) Memory request kube_pod_container_resource_requests_memory_bytes{namespace = \u0026#34;\u0026#34;} Memory usage relative to request sum by (container, pod, namespace) (container_memory_working_set_bytes{namespace = \u0026#34;pitc-rhe-serverless\u0026#34;,container!=\u0026#34;POD\u0026#34;,container!=\u0026#34;\u0026#34;}) / sum by (container, pod, namespace) (kube_pod_container_resource_requests_memory_bytes{namespace = \u0026#34;pitc-rhe-serverless\u0026#34;}) Prometheus Rules Create alerting rules in Prometheus:\n--- apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-custom-rules namespace: pitc-infra-monitoring spec: groups: - name: custom_node_monitoring rules: - alert: kubePersistentvolumeclaimStatus expr: | kube_persistentvolumeclaim_status_phase{phase!=\u0026#34;Bound\u0026#34;} * on (namespace) group_left() (kube_namespace_labels{label_kubernetes_io_metadata_name!=\u0026#34;\u0026#34;,label_pitc_sla=\u0026#34;prod\u0026#34;}) == 1 for: 24h annotations: message: \u0026#34;{{$labels.namespace}}: {{$labels.persistentvolumeclaim}} has phase {{$labels.phase}}\u0026#34; labels: severity: warning - alert: kubePersistentVolumeUnused expr: | max by(provider,platform,namespace,persistentvolumeclaim) ( kube_persistentvolumeclaim_status_phase{platform!=\u0026#34;openshift3\u0026#34;} ) * on(namespace) group_left() ( kube_namespace_labels{label_pitc_customer!=\u0026#34;fringebenefit\u0026#34;,label_pitc_ignore_unused_volume!=\u0026#34;true\u0026#34;} ) unless on(persistentvolumeclaim, namespace) count by(provider,platform,namespace,persistentvolumeclaim, instance, node, pod) ( avg_over_time(kube_pod_spec_volumes_persistentvolumeclaims_info[30d]) ) for: 24h annotations: message: \u0026#34;The PVC {{$labels.persistentvolumeclaim}} in {{$labels.namespace}} was not used for over 30 days\u0026#34; labels: severity: warning "},{"id":18,"href":"/doc/linux/bash/","title":"Bash","parent":"Linux","content":" Bash Port checking Check what\u0026rsquo;s running on port 8083.\nnetstat -ltnp | grep -w ':8083'\nRead pid startup command cat /proc/{PID}/cmdline\n"},{"id":19,"href":"/doc/cloud-events/","title":"Cloud-events","parent":"g1raffi.doc","content":""},{"id":20,"href":"/doc/","title":"g1raffi.doc","parent":"","content":""},{"id":21,"href":"/doc/kubernetes/","title":"Kubernetes","parent":"g1raffi.doc","content":""},{"id":22,"href":"/doc/linux/","title":"Linux","parent":"g1raffi.doc","content":""},{"id":23,"href":"/doc/quarkus/","title":"Quarkus","parent":"g1raffi.doc","content":""},{"id":24,"href":"/doc/tags/","title":"Tags","parent":"g1raffi.doc","content":""}]