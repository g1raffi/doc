[{"id":0,"href":"/doc/main/","title":"API Reference","parent":"g1raffi.doc","content":" Introduction "},{"id":1,"href":"/doc/quarkus/psql/psql/","title":"PSQL","parent":"Quarkus","content":" PSQL Quarkus Reactive psql client ./mvnw quarkus:add-extension -Dextensions=\u0026#34;reactive-pg-client\u0026#34; Starts automatically new devservice psql container with db/user/password = quarkus.\nroot@ac1e2f26032a:/# psql -U quarkus -d quarkus -h localhost -p 5432 Create InitDB Skript like @ApplicationScoped public class DBInit { private final PgPool client; private final boolean schemaCreate; private static final Logger log = Logger.getLogger(DBInit.class.getName()); public DBInit(PgPool client, @ConfigProperty(name = \u0026#34;myapp.schema.create\u0026#34;, defaultValue = \u0026#34;true\u0026#34;) boolean schemaCreate) { this.client = client; this.schemaCreate = schemaCreate; } void onStart(@Observes StartupEvent ev) { if (schemaCreate) { log.info(\u0026#34;Initializing Database\u0026#34;); initDb(); } } private void initDb() { client.query(\u0026#34;DROP TABLE IF EXISTS sensormeasurements\u0026#34;).execute() .flatMap(r -\u0026gt; client.query(\u0026#34;CREATE TABLE sensormeasurements (id SERIAL PRIMARY KEY, data DOUBLE PRECISION, time TIMESTAMP WITH TIME ZONE DEFAULT NOW()::timestamp)\u0026#34;).execute()) .flatMap(r -\u0026gt; client.query(\u0026#34;INSERT INTO sensormeasurements (data) VALUES (0.1)\u0026#34;).execute()) .flatMap(r -\u0026gt; client.query(\u0026#34;INSERT INTO sensormeasurements (data) VALUES (0.2)\u0026#34;).execute()) .flatMap(r -\u0026gt; client.query(\u0026#34;INSERT INTO sensormeasurements (data) VALUES (0.3)\u0026#34;).execute()) .flatMap(r -\u0026gt; client.query(\u0026#34;INSERT INTO sensormeasurements (data) VALUES (0.4)\u0026#34;).execute()) .await().indefinitely(); } } CRUD Operations example import io.smallrye.mutiny.Multi; import io.smallrye.mutiny.Uni; import io.vertx.mutiny.pgclient.PgPool; import io.vertx.mutiny.sqlclient.Row; import io.vertx.mutiny.sqlclient.RowSet; import io.vertx.mutiny.sqlclient.Tuple; import java.time.Instant; import java.time.ZoneOffset; public class SensorMeasurement { public Long id; public Double data; public Instant time; public SensorMeasurement() { this.data = Math.random(); this.time = Instant.now(); } public SensorMeasurement(Row row) { this.id = row.getLong(\u0026#34;id\u0026#34;); this.data = row.getDouble(\u0026#34;data\u0026#34;); this.time = Instant.from(row.getOffsetDateTime(\u0026#34;time\u0026#34;)); } public static Multi\u0026lt;SensorMeasurement\u0026gt; findAll(PgPool client) { return client.query(\u0026#34;SELECT id, data, time from sensormeasurements\u0026#34;).execute() .onItem().transformToMulti(set -\u0026gt; Multi.createFrom().iterable(set)) .onItem().transform(SensorMeasurement::new); } public static Uni\u0026lt;SensorMeasurement\u0026gt; findById(PgPool client, Long id) { return client.preparedQuery(\u0026#34;SELECT id, data, time from sensormeasurements where id = $1\u0026#34;).execute(Tuple.of(id)) .onItem().transform(RowSet::iterator) .onItem().transform(iterator -\u0026gt; iterator.hasNext() ? new SensorMeasurement(iterator.next()) : null); } public static Uni\u0026lt;SensorMeasurement\u0026gt; findLatest(PgPool client) { return client.query(\u0026#34;SELECT id, data, time from sensormeasurements ORDER BY time DESC LIMIT 1\u0026#34;).execute() .map(RowSet::iterator) .map(rowRowIterator -\u0026gt; rowRowIterator.hasNext() ? new SensorMeasurement(rowRowIterator.next()) : null); } public Uni\u0026lt;SensorMeasurement\u0026gt; save(PgPool client) { return client.preparedQuery(\u0026#34;INSERT INTO sensormeasurements (data, time) VALUES ($1, $2) RETURNING (id, data, time)\u0026#34;) .execute(Tuple.of(data, time.atOffset(ZoneOffset.UTC))) .onItem().transform(RowSet::iterator) .onItem().transform(iterator -\u0026gt; iterator.hasNext() ? this : null); } } Generic Map\u0026lt;String, String\u0026gt; jsonb types Add dependencies:\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive-jackson\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-flyway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-hibernate-orm-panache\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkiverse.hibernatetypes\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-hibernate-types\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-jdbc-postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Create migration skripts \u0026ldquo;/resources/db/migration\u0026rdquo;:\nV1.0.0__Model.sql:\nCREATE TABLE public.example ( id integer NOT NULL, content jsonb, CONSTRAINT id_pkey PRIMARY KEY (id) ) V1.0.1__Data.sql:\nINSERT INTO public.example (id, content) VALUES (1, \u0026#39;{ \u0026#34;field\u0026#34;: \u0026#34;value\u0026#34; }\u0026#39;); INSERT INTO public.example (id, content) VALUES (2, \u0026#39;{ \u0026#34;anotherField\u0026#34;: \u0026#34;anotherValue\u0026#34; }\u0026#39;); Create and define entity:\nimport io.quarkiverse.hibernate.types.json.JsonBinaryType; import io.quarkiverse.hibernate.types.json.JsonTypes; import io.quarkus.hibernate.orm.panache.PanacheEntityBase; import org.hibernate.annotations.Type; import org.hibernate.annotations.TypeDef; import org.hibernate.annotations.TypeDefs; import javax.persistence.Entity; import javax.persistence.Id; import java.util.Map; @Entity @TypeDefs({ @TypeDef(name = \u0026#34;jsonb\u0026#34;, typeClass = JsonBinaryType.class) }) public class Example extends PanacheEntityBase { @Id public int id; @Type(type = JsonTypes.JSON_BIN) public Map\u0026lt;String, String\u0026gt; content; } Enjoy!\n"},{"id":2,"href":"/doc/cloud-events/cloud-events/","title":"CloudEvents","parent":"Cloud-events","content":" CloudEvents Github\nCNCF Incubating project since 2018.\nSummary Standardization is a general need in all fields after some technique is widely used. In the last few years the trend for scalable, event-driven systems grew massively. Everybody and everything is communicating in events. And all systems face the same question at some point - what should our events look like. If you were smart enough you asked the question rather earlier than later. If not, at some point you will face the truth that you will have to refactor a lot to achieve consistency in your events throughout your distributed system. This is where you would have wished to know CloudEvents already.\nCloudEvents brings a specification to describe events in a common way. The common language increases consistency, accessibility and portability in distributed systems. Major programming languages like Java, Go, JavaScript, Ruby, Rust, Python have SDKs and APIs to implement CloudEvents in a simple way. At it\u0026rsquo;s core it will bring us a blueprint or language to define a set of metadata to describe the event.\nExample CloudEvent\n{ \u0026#34;specversion\u0026#34; : \u0026#34;1.0\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;com.github.pull_request.opened\u0026#34;, \u0026#34;source\u0026#34; : \u0026#34;https://github.com/cloudevents/spec/pull\u0026#34;, \u0026#34;subject\u0026#34; : \u0026#34;123\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;A234-1234-1234\u0026#34;, \u0026#34;time\u0026#34; : \u0026#34;2018-04-05T17:31:00Z\u0026#34;, \u0026#34;comexampleextension1\u0026#34; : \u0026#34;value\u0026#34;, \u0026#34;comexampleothervalue\u0026#34; : 5, \u0026#34;datacontenttype\u0026#34; : \u0026#34;text/xml\u0026#34;, \u0026#34;data\u0026#34; : \u0026#34;\u0026lt;much wow=\\\u0026#34;xml\\\u0026#34;/\u0026gt;\u0026#34; } Specification It all comes down to a handful attributes:\nRequired attributes:\nattribute type description id String Identifies the event (e.g. UUID) source URI-reference Identifies the context in which an event happened specversion String Version of the CloudEvents specification which the event uses type String Describes the type of event related to the originating occurrence Optional attributes:\nattribute type description datacontenttype String RFC 2046 Content type of data value subject String This describes the subject of the event in the context of the event producer (identified by source). In publish-subscribe scenarios, a subscriber will typically subscribe to events emitted by a source time Timestamp RFC 3339 Timestamp of when the occurrence happened In addition to the specification of CloudEvents itself, there are extensions giving extra flexibility for other meta fields to enrich your event. For example OpenTracing header fields can be added by the Distributed Tracing extension.\nImplementation Let\u0026rsquo;s try to get our hands dirty and test the CloudEvent specification to fire some events. We are going to use the Quarkus framework with the Smallrye reactive messaging extension.\nAs it just happens the Smallrye reactive messaging extension supports CloudEvents out of the box!\nWe create two new Quarkus projects:\nCreation of Quarkus projects\nmvn io.quarkus.platform:quarkus-maven-plugin:2.10.3.Final:create \\ -DprojectGroupId=org.acme \\ -DprojectArtifactId=data-producer \\ -Dextensions=\u0026#34;resteasy-reactive\u0026#34; mvn io.quarkus.platform:quarkus-maven-plugin:2.10.3.Final:create \\ -DprojectGroupId=org.acme \\ -DprojectArtifactId=data-consumer \\ -Dextensions=\u0026#34;resteasy-reactive\u0026#34; Remove the test classes and add the following extensions to your projects\u0026rsquo; pom.xml:\nDependencies in pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-smallrye-reactive-messaging-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-confluent-registry-avro\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.confluent\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kafka-avro-serializer\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;6.1.1\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;jakarta.ws.rs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jakarta.ws.rs-api\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;confluent\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://packages.confluent.io/maven/\u0026lt;/url\u0026gt; \u0026lt;snapshots\u0026gt; \u0026lt;enabled\u0026gt;false\u0026lt;/enabled\u0026gt; \u0026lt;/snapshots\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; For demonstration purposes we use a Avro-schema, which is be the most common approach to manage schemas with Kafka.\nThe Producer Create a Avro schema src/main/avro/SensorMeasurement.avsc with the following content:\n{ \u0026#34;namespace\u0026#34;: \u0026#34;org.acme\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;SensorMeasurement\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } ] } This represents a simple Java POJO to hold some information about measurements we want to emit.\nWe create a service org.acme.KafkaProducer which allows us to emit SensorMeasurements to a defined Channel measurements which we will connect to a Kafka Topic. This could look like the following:\npackage org.acme; import org.eclipse.microprofile.reactive.messaging.Channel; import org.eclipse.microprofile.reactive.messaging.Emitter; import org.eclipse.microprofile.reactive.messaging.Message; import javax.enterprise.context.ApplicationScoped; import javax.inject.Inject; @ApplicationScoped public class KafkaProducer { @Channel(\u0026#34;measurements\u0026#34;) @Inject Emitter\u0026lt;SensorMeasurement\u0026gt; sensorMeasurementEmitter; public void emitEvent(SensorMeasurement sensorMeasurement) { sensorMeasurementEmitter.send(Message.of(sensorMeasurement)); } } Then we alter the pre-generated REST resource to emit an event every time we receive a POST request to the endpoint /measurements:\npackage org.acme; import javax.ws.rs.POST; import javax.ws.rs.Path; import javax.ws.rs.core.Response; import java.util.Random; @Path(\u0026#34;/measurements\u0026#34;) public class MeasurementsResource { private final KafkaProducer kafkaProducer; public MeasurementsResource(KafkaProducer kafkaProducer) { this.kafkaProducer = kafkaProducer; } @POST public Response emitMeasurement() { SensorMeasurement measurement = SensorMeasurement.newBuilder().setData(new Random().nextDouble()).build(); kafkaProducer.emitEvent(measurement); return Response.ok().build(); } } Of course we need some configuration in the application.properties to emit the events to our Kafka broker:\nmp.messaging.outgoing.measurements.connector=smallrye-kafka mp.messaging.outgoing.measurements.value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer mp.messaging.outgoing.measurements.topic=measurements mp.messaging.outgoing.measurements.cloud-events-source=event-producer mp.messaging.outgoing.measurements.cloud-events-type=measurement-emitted mp.messaging.outgoing.measurements.cloud-events-subject=subject-123 And this is where the magic happens: Configuring the channel \u0026ldquo;measurements\u0026rdquo; with additional properties cloud-events-XXX will simply enrich the message envelope with the properties defined. If you don\u0026rsquo;t like the config approach and would rather enrich the message programmatically, I got you covered!\npublic void emitEvent(SensorMeasurement sensorMeasurement) { OutgoingCloudEventMetadata\u0026lt;Object\u0026gt; metadata = OutgoingCloudEventMetadata.builder() .withId(UUID.randomUUID().toString()) .withSource(URI.create(\u0026#34;event-producer\u0026#34;)) .withType(\u0026#34;measurement-emitted\u0026#34;) .withSubject(\u0026#34;subject-123\u0026#34;) .build(); sensorMeasurementEmitter.send(Message.of(sensorMeasurement).addMetadata(metadata)); } And that\u0026rsquo;s all you need to create events to our little system!\nThe consumer The data-consumer\u0026rsquo;s side of the system looks quite similar. Create a Avro schema src/main/avro/SensorMeasurement.avsc with the following content:\n{ \u0026#34;namespace\u0026#34;: \u0026#34;org.acme\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;SensorMeasurement\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;double\u0026#34; } ] } Instead of producing messages, we will simply define a listener on a channel connected to the same Kafka topic and print the events to the command line!\nCreate the EventListener org.acme.EventListener:\npackage org.acme; import io.smallrye.reactive.messaging.ce.IncomingCloudEventMetadata; import org.eclipse.microprofile.reactive.messaging.Incoming; import org.eclipse.microprofile.reactive.messaging.Message; import org.jboss.logging.Logger; import javax.enterprise.context.ApplicationScoped; import java.util.concurrent.CompletionStage; @ApplicationScoped public class EventListener { private final Logger logger = Logger.getLogger(EventListener.class); @Incoming(\u0026#34;measurements\u0026#34;) public CompletionStage\u0026lt;Void\u0026gt; consume(Message\u0026lt;SensorMeasurement\u0026gt; message) { IncomingCloudEventMetadata cloudEventMetadata = message.getMetadata(IncomingCloudEventMetadata.class).orElseThrow(() -\u0026gt; new IllegalArgumentException(\u0026#34;Expected a CloudEvent!\u0026#34;)); logger.infof(\u0026#34;Received Cloud Events (spec-version: %s): id: \u0026#39;%s\u0026#39;, source: \u0026#39;%s\u0026#39;, type: \u0026#39;%s\u0026#39;, subject: \u0026#39;%s\u0026#39;, payload-message: \u0026#39;%s\u0026#39; \u0026#34;, cloudEventMetadata.getSpecVersion(), cloudEventMetadata.getId(), cloudEventMetadata.getSource(), cloudEventMetadata.getType(), cloudEventMetadata.getSubject().orElse(\u0026#34;no subject\u0026#34;), message.getPayload()); return message.ack(); } } Add the following properties to the application.properties file:\nmp.messaging.incoming.measurements.connector=smallrye-kafka mp.messaging.incoming.measurements.topic=measurements quarkus.http.port=8081 And that\u0026rsquo;s all you need to have your application up and running! If you have Docker installed, starting the application will also start a mock Kafka broker and connect your applications automatically. If you don\u0026rsquo;t have Docker installed you will need to configure the connection to a Kafka broker yourself by adding the following property to the applications:\nkafka.bootstrap.servers=your-kafka-broker:9092 Test your events Start both of your applications in your favorite IDE or shell:\n./mvnw compile quarkus:dev Fire some requests against your producer and test your CloudEvents getting emitted and consumed!\n$ curl -X POST localhost:8080/measurements 2022-07-21 11:01:56,654 INFO [org.acm.EventListener] (vert.x-eventloop-thread-10) Received Cloud Events (spec-version: 1.0): id: \u0026#39;98d85610-6d8d-4943-b8ea-641c4940e148\u0026#39;, source: \u0026#39;event-producer\u0026#39;, type: \u0026#39;measurement-emitted\u0026#39;, subject: \u0026#39;subject-123\u0026#39;, payload-message: \u0026#39;{\u0026#34;data\u0026#34;: 0.12169099891061863}\u0026#39; Recap With the CloudEvents standard you can increase consistency, accessibility and portability of your microservice architecture. You can save time discussing what events should look like and increase efficiency by simply implementing your events and get stuff done!\n"},{"id":3,"href":"/doc/quarkus/junit/junit/","title":"JUnit","parent":"Quarkus","content":" Creating Dynamic Tests with JUnit 5 There are probably many projects in the Java ecosystem that are not relying on JUnit at their core. Migrating from JUnit 4 to JUnit 5 brings a lot of changes to the well known testing environment. It adopts the new features from Java 8+ and enables more different kinds of testing. In this small sample we are going to take a look at the new feature of Dynamic Tests in JUnit 5.\nInstead of using the standard approac to use staticly implemented tests at compile time, we can use a TestFactory to generate Dynamic Tests at runtime. Methods annotated with @TestFactory must return a Stream, Collection, Iterable or Iterator of DynamicTest instances, which will be the dynamically executed test during runtime. One of the main differences in Dynamic Tests\nThe standard tests annotated with @Test annotation are static tests which are fully specified at the compile time. A DynamicTest is a test generated during runtime. These tests are generated by a factory method annotated with the @TestFactory annotation.\nA @TestFactory method must return a Stream, Collection, Iterable, or Iterator of DynamicTest instances. Returning anything else will result in a JUnitException since the invalid return types cannot be detected at compile time. Apart from this, a @TestFactory method cannot be static or private.\nThe DynamicTests are executed differently than the standard @Tests and do not support lifecycle callbacks. Meaning, the @BeforeEach and the @AfterEach methods will not be called for the DynamicTests.\nEvent.java:\npackage io.quarkus.mcve.entity; import io.quarkiverse.hibernate.types.json.JsonBinaryType; import io.quarkiverse.hibernate.types.json.JsonTypes; import io.quarkus.hibernate.orm.panache.PanacheEntityBase; import org.hibernate.annotations.Type; import org.hibernate.annotations.TypeDef; import org.hibernate.annotations.TypeDefs; import javax.persistence.Entity; import javax.persistence.Id; import java.time.Instant; import java.util.Map; import java.util.UUID; @Entity @TypeDefs({ @TypeDef(name = \u0026#34;jsonb\u0026#34;, typeClass = JsonBinaryType.class) }) public class Event extends PanacheEntityBase { @Id public UUID id; public String description; public Instant timestamp; @Type(type = JsonTypes.JSON_BIN) public Map\u0026lt;String, String\u0026gt; details; @Type(type = JsonTypes.JSON_BIN) public Map\u0026lt;String, String\u0026gt; monitoring; } Configuration yaml file:\n/resources/application.yaml:\nquarkus: flyway: migrate-at-start: true hibernate-orm: database.generation: none testconfig: suites: - name: suite1 api: localhost:8080 request-body: \u0026#39;{\u0026#34;user\u0026#34;: \u0026#34;g1raffi\u0026#34;}\u0026#39; correlation-id: 1 assertions: - name: assertion1 from-state: FIRST_STATE to-state: SECOND_STATE max-time-ms: 3100 - name: assertion2 from-state: FIRST_STATE to-state: THIRD_STATE max-time-ms: 4100 TestConfiguration.java:\nThe test configuration files\npackage io.quarkus.mcve.config; import io.smallrye.config.ConfigMapping; import java.util.List; @ConfigMapping(prefix = \u0026#34;testconfig\u0026#34;) public interface TestConfiguration { List\u0026lt;TestSuite\u0026gt; suites(); } TestSuite.java:\npackage io.quarkus.mcve.config; import java.util.List; public interface TestSuite { String name(); String correlationId(); String api(); String requestBody(); List\u0026lt;TestAssertion\u0026gt; assertions(); } TestAssertion.java:\npackage io.quarkus.mcve.config; public interface TestAssertion { String name(); String fromState(); String toState(); Double maxTimeMs(); } DB Migration with Flyway:\n/resources/db/migration/V1.0.0__Data.sql:\nCREATE TABLE public.event ( id uuid NOT NULL, description text, timestamp timestamp, details jsonb, monitoring jsonb, CONSTRAINT id_pkey PRIMARY KEY (id) ); INSERT INTO public.event (id, description, \u0026#34;timestamp\u0026#34;, details, monitoring) VALUES (\u0026#39;11111111-1111-1111-1111-111111111111\u0026#39;, \u0026#39;description1\u0026#39;, \u0026#39;2022-09-06T12:08:42Z\u0026#39;, \u0026#39;{ \u0026#34;state\u0026#34;: \u0026#34;FIRST_STATE\u0026#34;, \u0026#34;correlation-id\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39;, \u0026#39;{}\u0026#39;); INSERT INTO public.event (id, description, \u0026#34;timestamp\u0026#34;, details, monitoring) VALUES (\u0026#39;44444444-4444-4444-4444-444444444444\u0026#39;, \u0026#39;description4\u0026#39;, \u0026#39;2022-09-06T12:08:45Z\u0026#39;, \u0026#39;{ \u0026#34;state\u0026#34;: \u0026#34;SECOND_STATE\u0026#34;, \u0026#34;correlation-id\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39;, \u0026#39;{}\u0026#39;); INSERT INTO public.event (id, description, \u0026#34;timestamp\u0026#34;, details, monitoring) VALUES (\u0026#39;55555555-5555-5555-5555-555555555555\u0026#39;, \u0026#39;description5\u0026#39;, \u0026#39;2022-09-06T12:08:46Z\u0026#39;, \u0026#39;{ \u0026#34;state\u0026#34;: \u0026#34;THIRD_STATE\u0026#34;, \u0026#34;correlation-id\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39;, \u0026#39;{}\u0026#39;); Finally the test class\ntest/java/\u0026hellip;/DynamicTestSuites.java:\npackage io.quarkus.mcve; import io.quarkus.mcve.config.TestAssertion; import io.quarkus.mcve.config.TestConfiguration; import io.quarkus.mcve.entity.Event; import io.quarkus.test.junit.QuarkusTest; import org.junit.jupiter.api.Assertions; import org.junit.jupiter.api.DynamicTest; import org.junit.jupiter.api.TestFactory; import javax.inject.Inject; import java.util.List; import java.util.Map; import java.util.function.Function; import java.util.stream.Collectors; import java.util.stream.Stream; import static io.restassured.RestAssured.given; import static org.hamcrest.CoreMatchers.is; @QuarkusTest public class DynamicTestSuites { @Inject TestConfiguration testConfig; @TestFactory public Stream\u0026lt;DynamicTest\u0026gt; testSuiteSize() { return testConfig.suites().stream() .map(testSuite -\u0026gt; DynamicTest.dynamicTest( \u0026#34;Running suite: \u0026#34; + testSuite.name(), () -\u0026gt; { // Fire Event makeRestCall(testSuite.api(), testSuite.requestBody()); // Sleep for Events to finish // Get Event Chain List\u0026lt;Event\u0026gt; eventChain = getEventChain(); // Make assertions for (TestAssertion assertion : testSuite.assertions()) { System.out.println(\u0026#34;maxTime: \u0026#34; + assertion.maxTimeMs()); System.out.println(\u0026#34;getTimeBetweenEvents: \u0026#34; + getTimeBetweenEvents(eventChain, assertion.fromState(), assertion.toState())); Assertions.assertTrue(assertion.maxTimeMs() \u0026gt; getTimeBetweenEvents(eventChain, assertion.fromState(), assertion.toState())); } } )); } private long getTimeBetweenEvents(List\u0026lt;Event\u0026gt; eventChain, String fromState, String toState) { Map\u0026lt;String, Event\u0026gt; stateMap = eventChain.stream().collect(Collectors.toMap( event -\u0026gt; event.details.get(\u0026#34;state\u0026#34;), Function.identity() )); System.out.println(\u0026#34;getTimeBetweenEvents called\u0026#34;); System.out.println(\u0026#34;fromState time: \u0026#34; + stateMap.get(fromState).timestamp.toEpochMilli()); System.out.println(\u0026#34;toState time: \u0026#34; + stateMap.get(toState).timestamp.toEpochMilli()); return stateMap.get(toState).timestamp.toEpochMilli() - stateMap.get(fromState).timestamp.toEpochMilli(); } private List\u0026lt;Event\u0026gt; getEventChain() { return Event.listAll(); } private void makeRestCall(String api, String requestBody) { } } \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive-jackson\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-flyway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-hibernate-orm-panache\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkiverse.hibernatetypes\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-hibernate-types\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-config-yaml\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-jdbc-postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-junit5\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.rest-assured\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rest-assured\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Tada!\n"},{"id":4,"href":"/doc/quarkus/reactive/","title":"Reactive","parent":"Quarkus","content":" Reactive World Emit periodic messages SomeEmitter.class\n@ConfigProperty(name = \u0026#34;application.polling.interval-seconds\u0026#34;, defaultValue = \u0026#34;900\u0026#34;) int pollingInterval; @Outgoing(\u0026#34;Trigger\u0026#34;) public Flowable\u0026lt;String\u0026gt; triggerUpdateEvent() { return Flowable.interval(pollingInterval, TimeUnit.SECONDS) .map(tick -\u0026gt; \u0026#34;triggered\u0026#34;); } @Incoming(\u0026#34;Trigger\u0026#34;) @Outgoing(\u0026#34;InboundReading\u0026#34;) public PublisherBuilder\u0026lt;List\u0026lt;E\u0026gt;\u0026gt; update(String triggered) { log.debug(\u0026#34;trigger received\u0026#34;); E e = result(); return ReactiveStreams.of(e); } Add dependencies pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-reactive-pg-client\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-flyway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-jdbc-postgresql\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-resteasy-reactive-jsonb\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Add dependencies to your code:\nReactive PgClient Create a @ApplicationScoped repository for the data access:\nimport io.smallrye.mutiny.Multi; import io.vertx.mutiny.pgclient.PgPool; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class DataRepository { private final PgPool pgPool; public DataRepository(PgPool pgPool) { this.pgPool = pgPool; } Multi\u0026lt;Data\u0026gt; findAllData() { return pgPool.query(\u0026#34;SELECT * FROM jsondata\u0026#34;) .execute() .onItem() .transformToMulti(i -\u0026gt; Multi.createFrom().iterable(i)) .map(i -\u0026gt; new Data(i.getUUID(\u0026#34;id\u0026#34;).toString(), i.getString(\u0026#34;json\u0026#34;))); } } Migration Create flyway migration script to initialize table and add dummy data:\nsrc/main/resources/db/migration/V1.0.0__Quarkus.sql\nCREATE TABLE IF NOT EXISTS jsondata ( id uuid PRIMARY KEY, json text ); INSERT INTO jsondata (id, json) VALUES (\u0026#39;21c23437-89d1-4774-bbbe-c286fb7c3afd\u0026#39;, \u0026#39;teststring\u0026#39;); Configuration Add configuration to enable auto migration:\napplication.property\nquarkus.flyway.migrate-at-start=true Rest Resource Alter the REST resource to return the object from the database:\nDataResource.java\npackage org.acme; import io.smallrye.mutiny.Multi; import javax.inject.Inject; import javax.ws.rs.GET; import javax.ws.rs.Path; import javax.ws.rs.Produces; import javax.ws.rs.core.MediaType; @Path(\u0026#34;/data\u0026#34;) public class DataResource { @Inject DataRepository dataRepository; @GET @Produces(MediaType.APPLICATION_JSON) public Multi\u0026lt;Data\u0026gt; hello() { return dataRepository.findAllData(); } } Data.java\npackage org.acme; public class Data { public String id; public String data; public Data() { } public Data(String id, String data) { this.id = id; this.data = data; } } "},{"id":5,"href":"/doc/linux/fedora/","title":"Fedora","parent":"Linux","content":" Switch to fedora Docker replaced with podman:\nNon-root problematic with podman and all the other docker-like resources are fixed by setting:\n# Install the required podman packages from dnf. If you\u0026#39;re not using rpm based # distro, replace with respective package manager sudo dnf install podman podman-docker # Enable the podman socket with Docker REST API systemctl --user enable podman.socket --now # Set the required envvars export DOCKER_HOST=unix:///run/user/${UID}/podman/podman.sock export TESTCONTAINERS_RYUK_DISABLED=true "},{"id":6,"href":"/doc/kubernetes/postgres/","title":"Postgres","parent":"Kubernetes","content":"To deploy and start ephemereal postgres container apply the following manifest:\n--- apiVersion: v1 kind: Service metadata: name: postgres labels: app: postgres spec: selector: app: postgres ports: - protocol: TCP port: 5432 targetPort: 5432 --- apiVersion: apps/v1 kind: Deployment metadata: name: postgres labels: app: postgres spec: replicas: 1 selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: containers: - name: postgres image: postgres:latest ports: - containerPort: 5432 env: - name: POSTGRES_PASSWORD value: password - name: POSTGRES_USER value: user - name: POSTGRES_DB value: db - name: PGDATA value: /var/lib/postgresql/data/pgdata volumeMounts: - mountPath: /var/lib/postgresql/data name: psqldata readOnly: false volumes: - name: psqldata emptyDir: {} "},{"id":7,"href":"/doc/linux/sed/","title":"Sed","parent":"Linux","content":" SED Inline replace user password files with sed:\nsed -rEi \u0026quot;s/.*(user.*):(.*)@.*/\\1,\\2/g\u0026quot; users.csv\n"},{"id":8,"href":"/doc/kubernetes/chaos-engineering/chaos-engineering/","title":"Chaos Engineering","parent":"Kubernetes","content":"In the world of distributed computing we face a lot of new problems. Requirements change from steady stability into distributed architectures consisting of a multitude of services. Coming from the familiar world we take several things as granted, often called as the eight fallacies of distributed computing:\nThe network is reliable There is zero latency Bandwidth is infinite The network is secure Topology never changes The network is homogeneous Consistent resource usage with no spikes All shared resources are available from all places With supercharging our microservice architectures by following devops and gitops principles, we can rollout as fast as possible. Using the ability to make production ready deployments every few hours / days, we need to have confidence in our system. We can improve confidence by using testing principles. Software sided tests verify the integrity of our code. Infrastucutre test verify the theoretical and practical representation of our infrastructure as code. If we even go further, we use these combined and test our system end to end automatically.\nThere is also the other side of the medal. Even with having all the tests described above, we still should be on our toes. Classical approaches of testing verify the desired and intended state / workflow of our systems. What about the unpredictable? What about chaos?\nPrinciples of chaos engineering Chaos Engineering describes the discipline of creating scanarios simulating turbulent conditions to build confidence in our systems robustness.\nEven when all of the individual services in a distributed system are functioning properly, the interactions between those services can cause unpredictable outcomes. Unpredictable outcomes, compounded by rare but disruptive real-world events that affect production environments, make these distributed systems inherently chaotic.\nAn empirical, systems-based approach addresses the chaos in distributed systems at scale and builds confidence in the ability of those systems to withstand realistic conditions.\nChaos in practice To specifically address the uncertainty of distributed systems at scale, Chaos Engineering can be thought of as the facilitation of experiments to uncover systemic weaknesses. These experiments follow four steps:\nStart by defining ‘steady state’ as some measurable output of a system that indicates normal behavior. Hypothesize that this steady state will continue in both the control group and the experimental group. Introduce variables that reflect real world events like servers that crash, hard drives that malfunction, network connections that are severed, etc. Try to disprove the hypothesis by looking for a difference in steady state between the control group and the experimental group. The harder it is to disrupt the steady state, the more confidence we have in the behavior of the system. If a weakness is uncovered, we now have a target for improvement before that behavior manifests in the system at large.\nADVANCED PRINCIPLES The following principles describe an ideal application of Chaos Engineering, applied to the processes of experimentation described above. The degree to which these principles are pursued strongly correlates to the confidence we can have in a distributed system at scale. Build a Hypothesis around Steady State Behavior\nFocus on the measurable output of a system, rather than internal attributes of the system. Measurements of that output over a short period of time constitute a proxy for the system’s steady state. The overall system’s throughput, error rates, latency percentiles, etc. could all be metrics of interest representing steady state behavior. By focusing on systemic behavior patterns during experiments, Chaos verifies that the system does work, rather than trying to validate how it works.\nVary Real-world Events Chaos variables reflect real-world events. Prioritize events either by potential impact or estimated frequency. Consider events that correspond to hardware failures like servers dying, software failures like malformed responses, and non-failure events like a spike in traffic or a scaling event. Any event capable of disrupting steady state is a potential variable in a Chaos experiment.\nRun Experiments in Production Systems behave differently depending on environment and traffic patterns. Since the behavior of utilization can change at any time, sampling real traffic is the only way to reliably capture the request path. To guarantee both authenticity of the way in which the system is exercised and relevance to the current deployed system, Chaos strongly prefers to experiment directly on production traffic.\nAutomate Experiments to Run Continuously Running experiments manually is labor-intensive and ultimately unsustainable. Automate experiments and run them continuously. Chaos Engineering builds automation into the system to drive both orchestration and analysis.\nMinimize Blast Radius Experimenting in production has the potential to cause unnecessary customer pain. While there must be an allowance for some short-term negative impact, it is the responsibility and obligation of the Chaos Engineer to ensure the fallout from experiments are minimized and contained.\nChaos Engineering is a powerful practice that is already changing how software is designed and engineered at some of the largest-scale operations in the world. Where other practices address velocity and flexibility, Chaos specifically tackles systemic uncertainty in these distributed systems. The Principles of Chaos provide confidence to innovate quickly at massive scales and give customers the high quality experiences they deserve.\nTest Environment Recommendations - how and where to run chaos tests Run the chaos tests continuously in your test pipelines: Software, systems, and infrastructure does change – and the condition/health of each can change pretty rapidly. A good place to run tests is in your CI/CD pipeline running on a regular cadence. Run the chaos tests manually to learn from the system: hen running a Chaos scenario or Fault tests, it is more important to understand how the system responds and reacts, rather than mark the execution as pass or fail. It is important to define the scope of the test before the execution to avoid some issues from masking others. Run the chaos tests in production environments or mimic the load in staging environments: As scary as a thought about testing in production is, production is the environment that users are in and traffic spikes/load are real. To fully test the robustness/resilience of a production system, running Chaos Engineering experiments in a production environment will provide needed insights. A couple of things to keep in mind: Minimize blast radius and have a backup plan in place to make sure the users and customers do not undergo downtime. Mimic the load in a staging environment in case Service Level Agreements are too tight to cover any downtime. Enable Observability: Chaos Engineering Without Observability … Is Just Chaos. Make sure to have logging and monitoring installed on the cluster to help with understanding the behaviour as to why it is happening. In case of running the tests in the CI where it is not humanly possible to monitor the cluster all the time, it is recommended to leverage Cerberus to capture the state during the runs and metrics collection in Kraken to store metrics long term even after the cluster is gone. Kraken ships with dashboards that will help understand API, Etcd and OpenShift cluster level stats and performance metrics. Pay attention to Prometheus alerts. Check if they are firing as expected. Run multiple chaos tests at once to mimic the production outages: For example, hogging both IO and Network at the same time instead of running them separately to observe the impact. You might have existing test cases, be it related to Performance, Scalability or QE. Run the chaos in the background during the test runs to observe the impact. Signaling feature in Kraken can help with coordinating the chaos runs i.e., start, stop, pause the scenarios based on the state of the other test jobs. "},{"id":9,"href":"/doc/kubernetes/chaos-engineering/litmus/litmus/","title":"Litmus","parent":"Kubernetes","content":" Install Litmus namespace based Installation Guide\nCreate scc:\nallowHostDirVolumePlugin: true allowHostIPC: false allowHostNetwork: false allowHostPID: true allowHostPorts: false allowPrivilegeEscalation: true allowPrivilegedContainer: true allowedCapabilities: null apiVersion: security.openshift.io/v1 defaultAddCapabilities: - NET_ADMIN - SYS_ADMIN fsGroup: type: MustRunAs kind: SecurityContextConstraints metadata: name: litmus priority: null readOnlyRootFilesystem: false requiredDropCapabilities: - KILL - MKNOD - SETUID - SETGID runAsUser: type: RunAsAny seLinuxContext: type: MustRunAs supplementalGroups: type: RunAsAny users: [] volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Bind Service accounts to SCC:\noc adm policy add-scc-to-user litmus -z litmus-server-account oc adm policy add-scc-to-user litmus -z chaos-mongodb oc adm policy add-scc-to-user litmus -z litmus-namespace-scope oc adm policy add-scc-to-user litmus -z argo-chaos Create override config when using OpenShift instead of plain Kubernetes:\ncat \u0026lt;\u0026lt;EOF \u0026gt; override-openshift.yaml portalScope: namespace portal.server.service.type: ClusterIP portal.frontend.service.type: ClusterIP openshift.route.enabled: true EOF Install helm release:\nhelm install chaos litmuschaos/litmus --namespace=pitc-rhe-litmus -f override-openshift.yaml Delete securityContext from mongodb Deployment, workflow-controller Deployment, event-tracker Deployment, subscriber Deployment, chaos-operator-ce Deployment, chaos-controller ReplicaSet and Deployment.template and frontend Deployment.template.\nCreate ChaosExperiment:\nkind: Workflow apiVersion: argoproj.io/v1alpha1 metadata: name: kill-random-pods-1664177150 namespace: pitc-rhe-litmus creationTimestamp: null labels: cluster_id: 3acb5850-bb73-4269-ac92-e7bc26d3547c subject: kill-random-pods_pitc-rhe-litmus workflow_id: 704f3fe2-b762-4b3b-9419-c1b427140ebf workflows.argoproj.io/controller-instanceid: 3acb5850-bb73-4269-ac92-e7bc26d3547c spec: templates: - name: custom-chaos inputs: {} outputs: {} metadata: {} steps: - - name: install-chaos-experiments template: install-chaos-experiments arguments: {} - - name: pod-delete-fi3 template: pod-delete-fi3 arguments: {} - name: install-chaos-experiments inputs: artifacts: - name: pod-delete-fi3 path: /tmp/pod-delete-fi3.yaml raw: data: \u0026gt; apiVersion: litmuschaos.io/v1alpha1 description: message: | Deletes a pod belonging to a deployment/statefulset/daemonset kind: ChaosExperiment metadata: name: pod-delete labels: name: pod-delete app.kubernetes.io/part-of: litmus app.kubernetes.io/component: chaosexperiment app.kubernetes.io/version: 2.12.0 spec: definition: scope: Namespaced permissions: - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - create - delete - get - list - patch - update - deletecollection - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - get - list - patch - update - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps verbs: - get - list - apiGroups: - \u0026#34;\u0026#34; resources: - pods/log verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - pods/exec verbs: - get - list - create - apiGroups: - apps resources: - deployments - statefulsets - replicasets - daemonsets verbs: - list - get - apiGroups: - apps.openshift.io resources: - deploymentconfigs verbs: - list - get - apiGroups: - \u0026#34;\u0026#34; resources: - replicationcontrollers verbs: - get - list - apiGroups: - argoproj.io resources: - rollouts verbs: - list - get - apiGroups: - batch resources: - jobs verbs: - create - list - get - delete - deletecollection - apiGroups: - litmuschaos.io resources: - chaosengines - chaosexperiments - chaosresults verbs: - create - list - get - patch - update - delete image: litmuschaos/go-runner:2.12.0 imagePullPolicy: Always args: - -c - ./experiments -name pod-delete command: - /bin/bash env: - name: TOTAL_CHAOS_DURATION value: \u0026#34;15\u0026#34; - name: RAMP_TIME value: \u0026#34;\u0026#34; - name: FORCE value: \u0026#34;true\u0026#34; - name: CHAOS_INTERVAL value: \u0026#34;5\u0026#34; - name: PODS_AFFECTED_PERC value: \u0026#34;\u0026#34; - name: LIB value: litmus - name: TARGET_PODS value: \u0026#34;\u0026#34; - name: NODE_LABEL value: \u0026#34;\u0026#34; - name: SEQUENCE value: parallel labels: name: pod-delete app.kubernetes.io/part-of: litmus app.kubernetes.io/component: experiment-job app.kubernetes.io/version: 2.12.0 outputs: {} metadata: {} container: name: \u0026#34;\u0026#34; image: litmuschaos/k8s:2.12.0 command: - sh - -c args: - kubectl apply -f /tmp/pod-delete-fi3.yaml -n {{workflow.parameters.adminModeNamespace}} \u0026amp;\u0026amp; sleep 30 resources: {} - name: pod-delete-fi3 inputs: artifacts: - name: pod-delete-fi3 path: /tmp/chaosengine-pod-delete-fi3.yaml raw: data: | apiVersion: litmuschaos.io/v1alpha1 kind: ChaosEngine metadata: namespace: \u0026#34;{{workflow.parameters.adminModeNamespace}}\u0026#34; generateName: pod-delete-fi3 labels: workflow_run_id: \u0026#34;{{workflow.uid}}\u0026#34; spec: appinfo: appns: pitc-rhe-litmus applabel: app=example-web-go appkind: deployment engineState: active chaosServiceAccount: litmus-admin experiments: - name: pod-delete spec: components: env: - name: TOTAL_CHAOS_DURATION value: \u0026#34;30\u0026#34; - name: CHAOS_INTERVAL value: \u0026#34;10\u0026#34; - name: FORCE value: \u0026#34;false\u0026#34; - name: PODS_AFFECTED_PERC value: \u0026#34;\u0026#34; probe: - name: go-example type: httpProbe mode: Continuous runProperties: probeTimeout: 10 retry: 10 interval: 10 probePollingInterval: 10 initialDelaySeconds: 10 stopOnFailure: false httpProbe/inputs: url: http://example:5000 insecureSkipVerify: true responseTimeout: 2 method: get: criteria: == responseCode: \u0026#34;200\u0026#34; outputs: {} metadata: labels: weight: \u0026#34;10\u0026#34; container: name: \u0026#34;\u0026#34; image: litmuschaos/litmus-checker:2.12.0 args: - -file=/tmp/chaosengine-pod-delete-fi3.yaml - -saveName=/tmp/engine-name resources: {} entrypoint: custom-chaos arguments: parameters: - name: adminModeNamespace value: pitc-rhe-litmus serviceAccountName: argo-chaos status: ? startedAt ? finishedAt "},{"id":10,"href":"/doc/argocd/","title":"Argocd","parent":"g1raffi.doc","content":" ArgoCD Create application argocd apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: rhe-serverless namespace: argocd spec: destination: namespace: ${NAMESPACE} server: # Clusterroute or Clustername e.g.: https://${CLUSTER_API} project: project_name source: path: src/main/openshift repoURL: https://${TOKEN_NAME}:${TOKEN_VALUE}@gitlab.puzzle.ch/some-repo.git - CreateNamespace=true\nIgnore certain properties When using ArgoCD with HPA or Autoscaled Resources (keda.sh), you can ignore certain fields like this:\napiVersion: argoproj.io/v1alpha1 kind: Application spec: # [...] ignoreDifferences: - group: \u0026#34;apps\u0026#34; kind: \u0026#34;Deployment\u0026#34; jsonPointers: - /spec/replicas syncPolicy: syncOptions: - RespectIgnoreDifferences=true "},{"id":11,"href":"/doc/eda/","title":"Eda","parent":"g1raffi.doc","content":" event driven architecture https://www.youtube.com/watch?v=uxPFoImWpBg\u0026t=7200s\n"},{"id":12,"href":"/doc/kubernetes/chaos-engineering/kraken/kraken/","title":"Kraken","parent":"Kubernetes","content":""},{"id":13,"href":"/doc/kubernetes/chaos-engineering/litmus/blog/litmus-blog/","title":"Litmus Blog","parent":"Kubernetes","content":" Chaos Engineering with Litmus Cloud native technologies allow us to bring supercharged architectures up and running in no time. Resources get smaller and more lightweight. Adding Infrastructure as Code and GitOps concepts we are enabled to rollout more frequently and more distributed than ever before. But are we confident in our systems? When will we start building more confidence in our system? Acknowledging that our systems should not be fault-less but rather fault-tolerant, will adopt our view enormously. In this article I will present how adding a little chaos to the mix will massively improve the confidence in the systems we build.\nPrinciples of Chaos Let\u0026rsquo;s talk a bit about this \u0026rsquo;little chaos\u0026rsquo; we can use to boost our confidence in the system. In distributed computing we have a common set of fallacies that we often just take as granted in our world:\nThe network is reliable There is zero latency Bandwidth is infinite The network is secure Topology never changes The network is homogeneous Consistent resource usage with no spikes All shared resources are available from all places We can see that most of these fallacies occur because we tend to ignore hazardous chaos interrupting our happy state. A lot of platform operators fear these, but they shouldn\u0026rsquo;t! They should embrace the chaos!\nWith the principles of chaos engineering, we try to simulate these events and gain confidence by proving that our system can withstand these disruptive situations. Despite being called chaos engineering, the discipline itself is very empricial.\nIn practice Defining an empirical approach to chaotic behaviour can follow these four steps:\nDefine a \u0026lsquo;steady state\u0026rsquo; with measurable output of your system in normal circumstances Create a hypothesis that your system will continue under chaotic behaviour the same as in normal every day business Introduce scenarios that reflect real world events (application crashes, memory shortage, \u0026hellip;) Verify your hypothesis by comparing the difference in the steady state and the behaviour in the chaotic world Resulting from these scenarios you will either gain confidence from the start, or you will detect weaknesses in your system that you will improve.\nTooling Before introducing a new tool to our already widely spread armory, I\u0026rsquo;d like to consider the CNCF Landscape. Luckily they already have a section for Chaos Engineering Tools ready:\nBecause I was already very familiar with Argo I chose Litmus going forward. Under the hood Litmus builds upon Argo to run scenarios as Workflows.\nInstallation Litmus on OpenShift The installation of the Litmus Chaos Center ist documented here. In the following I will show you how I installed Litmus via Helm Chart on OpenShift 4.10 in a namespaced manner.\nCreate the namespace you want your Litmus installation to be located:\noc new-project To override certain elements which differ in the Helm chart for the OpenShift flavor create the following file: ```shell cat \u0026lt;\u0026lt;EOF \u0026gt; override-openshift.yaml portalScope: namespace portal: server: authServer: securityContext: null graphqlServer: securityContext: null service: type: ClusterIP frontend: securityContext: null service: type: ClusterIP openshift: route: enabled: true mongodb: containerSecurityContext: enabled: false podSecurityContext: enabled: false EOF Finally install the Helm release:\nhelm install chaos litmuschaos/litmus --namespace=litmus -f override-openshift.yaml Helm should confirm your installation like the following:\nNAME: chaos LAST DEPLOYED: Wed Sep 28 10:53:05 2022 NAMESPACE: litmus STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing litmus 😀 Your release is named chaos and it\u0026#39;s installed to namespace: litmus. Visit https://docs.litmuschaos.io to find more info. Wait for your pods to get ready, and as soon as all pods reach their readiness, check out your new chaos control hub!\noc get route chaos-litmus-route -ogo-template=\u0026#39;{{ .spec.host }}\u0026#39; Log into the control hub with the credentials:\nUsername: admin Password: litmus Verify in the Menu under Chaos Delegates that there is one delegate in the scope Namespace called Self-Agent with status Active. When your self-agent Chaos Delegate is all set and ready, it\u0026rsquo;s time to create your first Chaos Scenario!\nYour very first scenario To generate chaos, we first have to create a ready state! Let\u0026rsquo;s create some simple deployments we can use as a steady-state:\napiVersion: apps/v1 kind: Deployment metadata: labels: app: example-web-go name: example-web-go spec: replicas: 4 selector: matchLabels: app: example-web-go template: metadata: labels: app: example-web-go spec: containers: - image: quay.io/acend/example-web-go:latest name: example-web-go resources: requests: cpu: 10m memory: 16Mi limits: cpu: 20m memory: 32Mi And a service to verify their health:\napiVersion: v1 kind: Service metadata: name: example namespace: litmus spec: ports: - port: 5000 protocol: TCP targetPort: 5000 selector: app: example-web-go sessionAffinity: None type: ClusterIP Head over to the Chaos Scenarios menu item and click Scedule a new Scenario, choose your Self-Agent as the agent and select that you would like to \u0026ldquo;Create a new Chaos Scenario using the experiments from ChaosHub\u0026rdquo; and select the default Litmus ChaosHub. Give your first scenario a name and description and head to the next page. This was all pretty default stuff. From now on it will get more interesting, I promise!\nIn the next screen we can create the Argo Workflow which will represent our Chaos Scenario:\nAdd a new Chaos Experiment and select from the tempate list \u0026ldquo;generic/pod-delete\u0026rdquo;. With the edit button you can edit your Chaos Experiment. Let\u0026rsquo;s update it first to delete the pods we just created with the selector app=example-web-go:\nAfter defining the selector you will have to define a probe, which will verify the state of the applicaiton. In this case we will simply define a probe which sends a HTTP GET request to http://example:5000/ (which is our service we created) and expects the return code to be equal 200:\nWhen you have added the probe, finish the experiment and the creation of your scenario. You can choose to schedule your Chaos Scenario right now!\nHead over to the main page and you will see (hopefully) that your first scenario was a success!\n"},{"id":14,"href":"/doc/kubernetes/hacky-windoof/","title":"Hacky Windoof","parent":"Kubernetes","content":" Windoof CP File from pod to windows in gitbash\noc exec PODNAME -c CONTAINER_NAME -- bash -c \u0026#34;base64 FILE\u0026#34; | base64 -d \u0026gt; localfile "},{"id":15,"href":"/doc/kubernetes/helm/helm/","title":"Helm","parent":"Kubernetes","content":" Helm Post install hook to test CRD readiness apiVersion: batch/v1 kind: Job metadata: name: \u0026#34;{{ .Release.Name }}\u0026#34; labels: app.kubernetes.io/managed-by: {{ .Release.Service | quote }} app.kubernetes.io/instance: {{ .Release.Name | quote }} app.kubernetes.io/version: {{ .Chart.AppVersion }} helm.sh/chart: \u0026#34;{{ .Chart.Name }}-{{ .Chart.Version }}\u0026#34; annotations: # This is what defines this resource as a hook. Without this line, the # job is considered part of the release. \u0026#34;helm.sh/hook\u0026#34;: post-install \u0026#34;helm.sh/hook-weight\u0026#34;: \u0026#34;-5\u0026#34; \u0026#34;helm.sh/hook-delete-policy\u0026#34;: hook-succeeded spec: template: metadata: name: \u0026#34;{{ .Release.Name }}\u0026#34; labels: app.kubernetes.io/managed-by: {{ .Release.Service | quote }} app.kubernetes.io/instance: {{ .Release.Name | quote }} helm.sh/chart: \u0026#34;{{ .Chart.Name }}-{{ .Chart.Version }}\u0026#34; spec: restartPolicy: Never containers: {{- range .Values.operators }} - name: post-install-{{ .name }} image: \u0026#34;openshift4/ose-cli\u0026#34; command: [\u0026#34;test=$(oc get {{ .crd }} | grep NAME \u0026amp;\u0026gt; /dev/null); while [ $? -ne 0 ]; do sleep 5; echo \u0026#34;waiting\u0026#34;; test=$(oc get {{ .crd }} | grep NAME \u0026amp;\u0026gt; /dev/null); done;\u0026#34;] {{- end }} "},{"id":16,"href":"/doc/kubernetes/istio/servicemesh/","title":"Servicemesh","parent":"Kubernetes","content":" Service Mesh Operator (Istio) Installing Operator Install ElasticSearch Operator Install Jaeger Operator Install Service Mesh Operator Install Kiali Operator Setup the control plane UI Method:\nCreate new ServiceMeshControlPlane:\nCreate namespace (e.g. istio-system) Openshift Web Console Installed Operators Openshift Service Mesh Istio ServiceMeshControlPlane Create new Control Plane Declarative Method:\nCreate file for ServiceMeshControlPlane (smcp):\napiVersion: maistra.io/v2 kind: ServiceMeshControlPlane metadata: finalizers: - maistra.io/istio-operator generation: 1 name: basic namespace: istio-system spec: grafana: enabled: true jaeger: install: storage: type: Memory kiali: enabled: true prometheus: enabled: true policy: type: Istiod profiles: - default telemetry: type: Istiod tracing: sampling: 10000 type: Jaeger version: v2.0 Create the ServiceMeshMemberRoll Create ServiceMeshMemberRoll (smmr):\nOnly projects registered per ServiceMeshMemberRoll will get included in the contorl plane. There must be a ServiceMeshMemberRoll named default in the project of the ServiceMeshControlPlane.\napiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default namespace: istio-system spec: members: # a list of projects joined into the service mesh # - your-project-name # - another-project-name Update and add the projects to the ServiceMeshMemberRoll and apply the resource. Opt in deployments into control plane Workload must opt-in to be tracked by the control plane. This happens with the annotation: \u0026ldquo;sidecar.istio.io/inject\u0026rdquo;:\u0026ldquo;true\u0026rdquo;. Patch the desired resources to add the annotation to the workload that should be tracked.\noc patch deployment DEPLOYMENTNAME -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;sidecar.istio.io/inject\u0026#34;:\u0026#34;true\u0026#34;}}}}}\u0026#39; --type=merge To remove services from the service mesh delete the namespace from the ServiceMeshMemberRoll:\n$ oc -n istio-system patch --type=\u0026#39;json\u0026#39; smmr default -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/members\u0026#34;, \u0026#34;value\u0026#34;:[\u0026#34;\u0026#39;\u0026#34;NAMESPACE_TO_REMOVE\u0026#34;\u0026#39;\u0026#34;]}]\u0026#39; To add services to the service mesh add the namespace to the ServiceMeshMemberRoll:\noc -n istio-system patch --type=\u0026#39;json\u0026#39; smmr default -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/members\u0026#34;, \u0026#34;value\u0026#34;:[\u0026#34;\u0026#39;\u0026#34;NAMESPACE_TO_ADD\u0026#34;\u0026#39;\u0026#34;]}]\u0026#39; Getting applications to run in a service mesh Add the application to the ServiceMeshMemberRoll:\noc -n istio-system patch --type=\u0026#39;json\u0026#39; smmr default -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/members\u0026#34;, \u0026#34;value\u0026#34;:[\u0026#34;\u0026#39;\u0026#34;NAMESPACE_TO_ADD\u0026#34;\u0026#39;\u0026#34;]}]\u0026#39; Patch your Workload to opt-in with sidecar enrollment:\noc patch deployment DEPLOYMENTNAME -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;sidecar.istio.io/inject\u0026#34;:\u0026#34;true\u0026#34;}}}}}\u0026#39; --type=merge "},{"id":17,"href":"/doc/kubernetes/openshift/amq/amq/","title":"Amq","parent":"Kubernetes","content":" AMQ Openshift Setup Links Blog Red Hat\nInstall Operator\nCheck and install resources.\nConfiguration Almost all aspects of the broker on openshift are configured via the CR available. Most settings can be applied in the main resource ActiveMQArtemis.\nAddresses Adresses can be configured with the ActiveMQArtemisAddress custom resource. An example can be found here.\nBroker The broker will be configured with the ActiveMQArtemis custom resouce. Example.\nSecurit** Users, Groups and their roles and access strategies to addresses / queues can be defined in the ActiveMQArtemisSecurity custom resource.\nHA Setup Broker On Kubernetes, broker HA is achieved through health checks and container restarts. On-premise, the broker HA is achieved through master/slave (shared store or replication). When replication is used, the slave will already hold the queues in memory, and therefore is pretty much ready to go in case of failover. With shared storage, when the slave gets hold of the lock, then the queues need to be read from the journals ahead of the slave takeover. The time for a shared storage slave to take over will be dependent on the number and size of messages in the journal.\nWhen we talk about broker HA, it comes down to an active-passive failover mechanism (with Kubernetes being an exception). But Artemis also has an active-active clustering mechanism used primarily for scalability rather than HA. In active-active clustering, every message belongs to only one broker, and losing an active broker will make its messages also unaccessible—but a positive side effect of that issue is that the broker infrastructure is still up and functioning. Clients can use active instances and exchange messages with the drawback of temporarily not accessing the messages that are in the failed broker. To sum up, active-active clustering is primarily for scalability, but it also partially improves the availability with temporary message unavailability. Load balancer\nIf there is a load balancer, prefer one that is already HA in the organization, such as F5s. If Qpid is used, you will need two or more active instances for high availability. Clients\nThis is probably the easiest part, as most customers will already run the client services in redundantly HA fashion, which means two or more instances of consumers and producers most of the time. A side effect of running multiple consumers is that message ordering is not guaranteed. This is where message groups and exclusive consumers can be used.\nScalability Scalability is relatively easier to achieve with Artemis. Primarily, there are two approaches to scaling the message broker. Active-active clustering\nCreate a single logical broker cluster that is scaled transparently from the clients. This can be three masters and three slaves (replication or shared storage doesn’t matter) to start with, which means that clients can use any of the masters to produce and consume the messages. The broker will perform load balancing and message distributions. Such a messaging infrastructure is scalable and supports many queues and topics with different messaging patterns. Artemis can handle large and small messages effectively, so there is no need for using separate broker clusters depending on the message size either.\nA few of the consequences of active-active clustering are:\nMessage ordering is not preserved. Message grouping needs to be clustered. Scaling down requires message draining. Browsing the brokers and the queues is not centralized. Mutual TLS (no istia) Client side When a client tries to connect to a broker Pod in your deployment, the verifyHost option in the client connection URL determines whether the client compares the Common Name (CN) of the broker’s certificate to its host name, to verify that they match. The client performs this verification if you specify verifyHost=true or similar in the client connection URL.\nBroker side Generate a self-signed certificate for the broker key store.\n$ keytool -genkey -alias broker -keyalg RSA -keystore ~/broker.ks\nExport the certificate from the broker key store, so that it can be shared with clients. Export the certificate in the Base64-encoded .pem format. For example:\n$ keytool -export -alias broker -keystore ~/broker.ks -file ~/broker_cert.pem\nOn the client, create a client trust store that imports the broker certificate.\n$ keytool -import -alias broker -keystore ~/client.ts -file ~/broker_cert.pem\nOn the client, generate a self-signed certificate for the client key store.\n$ keytool -genkey -alias broker -keyalg RSA -keystore ~/client.ks\nOn the client, export the certificate from the client key store, so that it can be shared with the broker. Export the certificate in the Base64-encoded .pem format. For example:\n$ keytool -export -alias broker -keystore ~/client.ks -file ~/client_cert.pem\nCreate a broker trust store that imports the client certificate.\n$ keytool -import -alias broker -keystore ~/broker.ts -file ~/client_cert.pem\nLog in to OpenShift Container Platform as an administrator. For example:\n$ oc login -u system:admin\nSwitch to the project that contains your broker deployment. For example:\n$ oc project \u0026lt;my_openshift_project\u0026gt;\nCreate a secret to store the TLS credentials. For example:\n$ oc create secret generic my-tls-secret \\ --from-file=broker.ks=~/broker.ks \\ --from-file=client.ts=~/broker.ts \\ --from-literal=keyStorePassword=\u0026lt;password\u0026gt; \\ --from-literal=trustStorePassword=\u0026lt;password\u0026gt; Note\nWhen generating a secret, OpenShift requires you to specify both a key store and a trust store. The trust store key is generically named client.ts. For two-way TLS between the broker and a client, you must generate a secret that includes the broker trust store, because this holds the client certificate. Therefore, in the preceding step, the value that you specify for the client.ts key is actually the broker trust store file.\nLink the secret to the service account that you created when installing the Operator. For example:\n$ oc secrets link sa/amq-broker-operator secret/my-tls-secret\nSpecify the secret name in the sslSecret parameter of your secured acceptor or connector. For example:\nspec: ... acceptors: - name: my-acceptor protocols: amqp,openwire port: 5672 sslEnabled: true sslSecret: my-tls-secret expose: true connectionsAllowed: 5 ... "},{"id":18,"href":"/doc/kubernetes/openshift/amq/consumer/README/","title":"Readme","parent":"Kubernetes","content":" consumer Project This project uses Quarkus, the Supersonic Subatomic Java Framework.\nIf you want to learn more about Quarkus, please visit its website: https://quarkus.io/ .\nRunning the application in dev mode You can run your application in dev mode that enables live coding using:\n./mvnw compile quarkus:dev NOTE: Quarkus now ships with a Dev UI, which is available in dev mode only at http://localhost:8080/q/dev/.\nPackaging and running the application The application can be packaged using:\n./mvnw package It produces the quarkus-run.jar file in the target/quarkus-app/ directory. Be aware that it’s not an über-jar as the dependencies are copied into the target/quarkus-app/lib/ directory.\nThe application is now runnable using java -jar target/quarkus-app/quarkus-run.jar.\nIf you want to build an über-jar, execute the following command:\n./mvnw package -Dquarkus.package.type=uber-jar The application, packaged as an über-jar, is now runnable using java -jar target/*-runner.jar.\nCreating a native executable You can create a native executable using:\n./mvnw package -Pnative Or, if you don\u0026rsquo;t have GraalVM installed, you can run the native executable build in a container using:\n./mvnw package -Pnative -Dquarkus.native.container-build=true You can then execute your native executable with: ./target/consumer-1.0.0-SNAPSHOT-runner\nIf you want to learn more about building native executables, please consult https://quarkus.io/guides/maven-tooling.\nRelated Guides SmallRye Reactive Messaging - AMQP Connector (guide): Connect to AMQP with Reactive Messaging Provided Code Reactive Messaging codestart Use SmallRye Reactive Messaging\nRelated Apache AMQP 1.0 guide section\u0026hellip;\n"},{"id":19,"href":"/doc/kubernetes/openshift/amq/producer/README/","title":"Readme","parent":"Kubernetes","content":" producer Project This project uses Quarkus, the Supersonic Subatomic Java Framework.\nIf you want to learn more about Quarkus, please visit its website: https://quarkus.io/ .\nRunning the application in dev mode You can run your application in dev mode that enables live coding using:\n./mvnw compile quarkus:dev NOTE: Quarkus now ships with a Dev UI, which is available in dev mode only at http://localhost:8080/q/dev/.\nPackaging and running the application The application can be packaged using:\n./mvnw package It produces the quarkus-run.jar file in the target/quarkus-app/ directory. Be aware that it’s not an über-jar as the dependencies are copied into the target/quarkus-app/lib/ directory.\nThe application is now runnable using java -jar target/quarkus-app/quarkus-run.jar.\nIf you want to build an über-jar, execute the following command:\n./mvnw package -Dquarkus.package.type=uber-jar The application, packaged as an über-jar, is now runnable using java -jar target/*-runner.jar.\nCreating a native executable You can create a native executable using:\n./mvnw package -Pnative Or, if you don\u0026rsquo;t have GraalVM installed, you can run the native executable build in a container using:\n./mvnw package -Pnative -Dquarkus.native.container-build=true You can then execute your native executable with: ./target/producer-1.0.0-SNAPSHOT-runner\nIf you want to learn more about building native executables, please consult https://quarkus.io/guides/maven-tooling.\nRelated Guides SmallRye Reactive Messaging - AMQP Connector (guide): Connect to AMQP with Reactive Messaging Provided Code Reactive Messaging codestart Use SmallRye Reactive Messaging\nRelated Apache AMQP 1.0 guide section\u0026hellip;\n"},{"id":20,"href":"/doc/kubernetes/openshift/crc/","title":"CRC","parent":"Kubernetes","content":" Code Ready Containers Delete existing cluster\n# $ crc delete Setup and start cluster\n# $ crc setup $ crc start "},{"id":21,"href":"/doc/kubernetes/openshift/openshift/","title":"Openshift","parent":"Kubernetes","content":" OpenShift Go Templates Get route host\ncurl $(oc get route ROUTE_NAME -o go-template=\u0026#39;{{(index .status.ingress 0).host}}\u0026#39;)/data Compare ready and desired pod count\n[[ $(oc -n NAMESPACE get dc DC_NAME -o go-template=\u0026#39;{{.status.readyReplicas}}\u0026#39;) == $(oc -n NAMESPACE get dc DC_NAME -o go-template=\u0026#39;{{.status.replicas}}\u0026#39;) ]] Compare environment variables of pods\n[[ $(oc get pod data-consumer-75f69c845d-564bq -o go-template=\u0026#39;{{(index (index .spec.containers 0).env 1)}}\u0026#39;) == $(oc get pod data-producer-74f5d89975-vwxhw -o go-template=\u0026#39;{{(index (index .spec.containers 0).env 0)}}\u0026#39;) ]] Simple automated apply environment script #!/bin/bash echo \u0026#34;You are currently on project:\u0026#34; echo \u0026#34;-------------\u0026#34; oc status | head -1 echo \u0026#34;-------------\u0026#34; echo \u0026#34;Are you sure you want to apply all resources for environment \u0026#39;$1\u0026#39;?\u0026#34; echo \u0026#34;Press \u0026lt;Enter\u0026gt; to do so. Or press \u0026lt;Ctrl\u0026gt;+\u0026lt;C\u0026gt; to abort\u0026#34; read DUMMY ENVIRONMENT=$1 SECRETS=\u0026#34;artemis-credentials db-credentials jwt-tokens postgres-credentials\u0026#34; INFRASTRUCTURE=\u0026#34;routes artemis db-backup waf service-monitor-artemis-activemq service-monitor-postgresql service-monitor-spring-boot\u0026#34; APPS=\u0026#34;application\u0026#34; STATIC=\u0026#34;\u0026#34; DIR=\u0026#34;$(cd \u0026#34;$(dirname \u0026#34;${BASH_SOURCE[0]}\u0026#34; )\u0026#34; \u0026amp;\u0026amp; pwd)\u0026#34; if [ -z \u0026#34;$ENVIRONMENT\u0026#34; ]; then echo \u0026#39;Usage: ./apply-env.sh ENVIRONMENT\u0026#39; exit 1 fi if [ ! -z \u0026#34;$2\u0026#34; ]; then APPS=$2 fi ## configuration script for the environment ## # apply all secrets echo \u0026#39;Applying all secrets and configurations\u0026#39; for secret in ${SECRETS}; do if [ -f \u0026#34;${DIR}/../template/secret-${secret}.yml\u0026#34; ]; then oc get secret ${secret} || oc process -f ${DIR}/../template/secret-${secret}.yml --param-file \\ ${DIR}/../environments/env.yml --ignore-unknown-parameters \\ | oc apply -f - fi done echo \u0026#39;Applying all infrastructure resources\u0026#39; for infra in ${INFRASTRUCTURE}; do if [ -f \u0026#34;${DIR}/../template/infra-${infra}.yml\u0026#34; ]; then oc process -f ${DIR}/../template/infra-${infra}.yml --param-file \\ ${DIR}/../environments/env.yml --ignore-unknown-parameters \\ | oc apply -f - fi done echo \u0026#39;Applying all application resources\u0026#39; for app in ${APPS}; do if [ -f \u0026#34;${DIR}/../template/app-${app}.yml\u0026#34; ]; then oc process -f ${DIR}/../template/app-${app}.yml --param-file \\ ${DIR}/../environments/env.yml --ignore-unknown-parameters \\ | oc apply -f - fi done echo \u0026#39;Applying all static resources\u0026#39; for stat in ${STATIC}; do if [ -f \u0026#34;${DIR}/../static/${stat}.yml\u0026#34; ]; then oc apply -f ${DIR}/../static/${stat}.yml fi done echo \u0026#39;Done!\u0026#39; Use internal field in resource - name: WATCH_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name Create SA KUBECONFIG Create SA:\noc create serviceaccount github-deployer (Optional) Add Role and RoleBinding for deployment rights\nCreate local KUBECONFIG:\nTOKEN=$(oc sa get-token github-deployer) export KUBECONFIG=github-deployer.kubeconfig oc login --server=$(oc config view --minify -o jsonpath=\u0026#39;{.clusters[*].cluster.server}\u0026#39;) --token=$TOKEN unset KUBECONFIG The KUBECONFIG is generated in the location github-deployer.kubeconfig.\n"},{"id":22,"href":"/doc/kubernetes/prometheus/","title":"Prometheus","parent":"Kubernetes","content":" Prometheus Get CPU and Memory usage of kubernetes pods Get effecitve cpu usage of pod rate(conatiner_cpu_usage_seconds_total{container_name != \u0026#34;\u0026#34;, namespace = \u0026#34;\u0026#34;}[5m]) Cpu requests kube_pod_container_resource_requests_cpu_cores{namespace = \u0026#34;\u0026#34;} Effective memory usage sum_by(pod_name, instance)(container_memory_working_set_bytes{namespace=\u0026#34;\u0026#34;} / 1024^3) Memory request kube_pod_container_resource_requests_memory_bytes{namespace = \u0026#34;\u0026#34;} Memory usage relative to request sum by (container, pod, namespace) (container_memory_working_set_bytes{namespace = \u0026#34;pitc-rhe-serverless\u0026#34;,container!=\u0026#34;POD\u0026#34;,container!=\u0026#34;\u0026#34;}) / sum by (container, pod, namespace) (kube_pod_container_resource_requests_memory_bytes{namespace = \u0026#34;pitc-rhe-serverless\u0026#34;}) Prometheus Rules Create alerting rules in Prometheus:\n--- apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-custom-rules namespace: pitc-infra-monitoring spec: groups: - name: custom_node_monitoring rules: - alert: kubePersistentvolumeclaimStatus expr: | kube_persistentvolumeclaim_status_phase{phase!=\u0026#34;Bound\u0026#34;} * on (namespace) group_left() (kube_namespace_labels{label_kubernetes_io_metadata_name!=\u0026#34;\u0026#34;,label_pitc_sla=\u0026#34;prod\u0026#34;}) == 1 for: 24h annotations: message: \u0026#34;{{$labels.namespace}}: {{$labels.persistentvolumeclaim}} has phase {{$labels.phase}}\u0026#34; labels: severity: warning - alert: kubePersistentVolumeUnused expr: | max by(provider,platform,namespace,persistentvolumeclaim) ( kube_persistentvolumeclaim_status_phase{platform!=\u0026#34;openshift3\u0026#34;} ) * on(namespace) group_left() ( kube_namespace_labels{label_pitc_customer!=\u0026#34;fringebenefit\u0026#34;,label_pitc_ignore_unused_volume!=\u0026#34;true\u0026#34;} ) unless on(persistentvolumeclaim, namespace) count by(provider,platform,namespace,persistentvolumeclaim, instance, node, pod) ( avg_over_time(kube_pod_spec_volumes_persistentvolumeclaims_info[30d]) ) for: 24h annotations: message: \u0026#34;The PVC {{$labels.persistentvolumeclaim}} in {{$labels.namespace}} was not used for over 30 days\u0026#34; labels: severity: warning "},{"id":23,"href":"/doc/kubernetes/vault/vault/","title":"Vault","parent":"Kubernetes","content":" Vault Extract Kubernets Secrets and POST to Vault #!/bin/bash echo \u0026#34;--------------------------------------------------------------\u0026#34; echo \u0026#34;OpenShift Vault Importer\u0026#34; echo \u0026#34;--------------------------------------------------------------\u0026#34; read -p \u0026#34;Enter vault address [default: \\$VAULT_ADDRESS]: \u0026#34; addr addr=${addr:-$VAULT_ADDRESS} read -sp \u0026#34;Enter vault token [default: \\$VAULT_TOKEN]: \u0026#34; token token=${token:-$VAULT_TOKEN} echo \u0026#34;\u0026#34; read -p \u0026#34;Enter Vault Namespace [default: VAULT NAMESPACE]: \u0026#34; vns vns=${vns:-VAULT NAMESPACE} read -p \u0026#34;Enter OpenShift Namespace: \u0026#34; namespace read -p \u0026#34;Enter Secret name: \u0026#34; secret echo \u0026#39;{\u0026#34;data\u0026#34;:\u0026#39; \u0026gt; request.json oc get -n $namespace secret $secret -ojsonpath=\u0026#39;{.data}\u0026#39; \u0026gt;\u0026gt; request.json echo \u0026#39;}\u0026#39; \u0026gt;\u0026gt; request.json echo \u0026#34;$(cat request.json)\u0026#34; read -p \u0026#34;Want to apply this secret? [CTRL-C to abort, ENTER to continue] \u0026#34; yes curl -vk \\ -H \u0026#34;X-Vault-Token: $token\u0026#34; \\ -H \u0026#34;X-Vault-Namespace: $vns\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -X POST \\ -d @request.json \\ \u0026#34;$addr/v1/general-secrets/data/$secret\u0026#34; rm request.json "},{"id":24,"href":"/doc/linux/bash/","title":"Bash","parent":"Linux","content":" Bash Port checking Check what\u0026rsquo;s running on port 8083.\nnetstat -ltnp | grep -w ':8083'\nRead pid startup command cat /proc/{PID}/cmdline\n"},{"id":25,"href":"/doc/cloud-events/","title":"Cloud-events","parent":"g1raffi.doc","content":""},{"id":26,"href":"/doc/","title":"g1raffi.doc","parent":"","content":""},{"id":27,"href":"/doc/kubernetes/","title":"Kubernetes","parent":"g1raffi.doc","content":""},{"id":28,"href":"/doc/linux/","title":"Linux","parent":"g1raffi.doc","content":""},{"id":29,"href":"/doc/quarkus/","title":"Quarkus","parent":"g1raffi.doc","content":""},{"id":30,"href":"/doc/tags/","title":"Tags","parent":"g1raffi.doc","content":""}]