<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>g1raffi.doc</title><link>/doc/</link><description>Recent content on g1raffi.doc</description><generator>Hugo -- gohugo.io</generator><atom:link href="/doc/index.xml" rel="self" type="application/rss+xml"/><item><title>API Reference</title><link>/doc/main/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/main/</guid><description> Introduction</description></item><item><title>PSQL</title><link>/doc/quarkus/psql/psql/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/quarkus/psql/psql/</guid><description>PSQL Quarkus Reactive psql client ./mvnw quarkus:add-extension -Dextensions=&amp;#34;reactive-pg-client&amp;#34; Starts automatically new devservice psql container with db/user/password = quarkus.
root@ac1e2f26032a:/# psql -U quarkus -d quarkus -h localhost -p 5432 Create InitDB Skript like @ApplicationScoped public class DBInit { private final PgPool client; private final boolean schemaCreate; private static final Logger log = Logger.getLogger(DBInit.class.getName()); public DBInit(PgPool client, @ConfigProperty(name = &amp;#34;myapp.schema.create&amp;#34;, defaultValue = &amp;#34;true&amp;#34;) boolean schemaCreate) { this.client = client; this.schemaCreate = schemaCreate; } void onStart(@Observes StartupEvent ev) { if (schemaCreate) { log.</description></item><item><title>CloudEvents</title><link>/doc/cloud-events/cloud-events/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/cloud-events/cloud-events/</guid><description>CloudEvents Github
CNCF Incubating project since 2018.
Summary Standardization is a general need in all fields after some technique is widely used. In the last few years the trend for scalable, event-driven systems grew massively. Everybody and everything is communicating in events. And all systems face the same question at some point - what should our events look like. If you were smart enough you asked the question rather earlier than later.</description></item><item><title>JUnit</title><link>/doc/quarkus/junit/junit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/quarkus/junit/junit/</guid><description>Creating Dynamic Tests with JUnit 5 There are probably many projects in the Java ecosystem that are not relying on JUnit at their core. Migrating from JUnit 4 to JUnit 5 brings a lot of changes to the well known testing environment. It adopts the new features from Java 8+ and enables more different kinds of testing. In this small sample we are going to take a look at the new feature of Dynamic Tests in JUnit 5.</description></item><item><title>Reactive</title><link>/doc/quarkus/reactive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/quarkus/reactive/</guid><description>Reactive World Emit periodic messages SomeEmitter.class
@ConfigProperty(name = &amp;#34;application.polling.interval-seconds&amp;#34;, defaultValue = &amp;#34;900&amp;#34;) int pollingInterval; @Outgoing(&amp;#34;Trigger&amp;#34;) public Flowable&amp;lt;String&amp;gt; triggerUpdateEvent() { return Flowable.interval(pollingInterval, TimeUnit.SECONDS) .map(tick -&amp;gt; &amp;#34;triggered&amp;#34;); } @Incoming(&amp;#34;Trigger&amp;#34;) @Outgoing(&amp;#34;InboundReading&amp;#34;) public PublisherBuilder&amp;lt;List&amp;lt;E&amp;gt;&amp;gt; update(String triggered) { log.debug(&amp;#34;trigger received&amp;#34;); E e = result(); return ReactiveStreams.of(e); } Add dependencies pom.xml
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;quarkus-reactive-pg-client&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;quarkus-flyway&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;quarkus-jdbc-postgresql&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;quarkus-resteasy-reactive&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;quarkus-resteasy-reactive-jsonb&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; Add dependencies to your code:</description></item><item><title>Fedora</title><link>/doc/linux/fedora/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/linux/fedora/</guid><description> Switch to fedora Docker replaced with podman:
Non-root problematic with podman and all the other docker-like resources are fixed by setting:
# Install the required podman packages from dnf. If you&amp;#39;re not using rpm based # distro, replace with respective package manager sudo dnf install podman podman-docker # Enable the podman socket with Docker REST API systemctl --user enable podman.socket --now # Set the required envvars export DOCKER_HOST=unix:///run/user/${UID}/podman/podman.sock export TESTCONTAINERS_RYUK_DISABLED=true</description></item><item><title>Postgres</title><link>/doc/kubernetes/postgres/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/postgres/</guid><description>To deploy and start ephemereal postgres container apply the following manifest:
--- apiVersion: v1 kind: Service metadata: name: postgres labels: app: postgres spec: selector: app: postgres ports: - protocol: TCP port: 5432 targetPort: 5432 --- apiVersion: apps/v1 kind: Deployment metadata: name: postgres labels: app: postgres spec: replicas: 1 selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: containers: - name: postgres image: postgres:latest ports: - containerPort: 5432 env: - name: POSTGRES_PASSWORD value: password - name: POSTGRES_USER value: user - name: POSTGRES_DB value: db - name: PGDATA value: /var/lib/postgresql/data/pgdata volumeMounts: - mountPath: /var/lib/postgresql/data name: psqldata readOnly: false volumes: - name: psqldata emptyDir: {}</description></item><item><title>Sed</title><link>/doc/linux/sed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/linux/sed/</guid><description>SED Inline replace user password files with sed:
sed -rEi &amp;quot;s/.*(user.*):(.*)@.*/\1,\2/g&amp;quot; users.csv</description></item><item><title>Chaos Engineering</title><link>/doc/kubernetes/chaos-engineering/chaos-engineering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/chaos-engineering/chaos-engineering/</guid><description>In the world of distributed computing we face a lot of new problems. Requirements change from steady stability into distributed architectures consisting of a multitude of services. Coming from the familiar world we take several things as granted, often called as the eight fallacies of distributed computing:
The network is reliable There is zero latency Bandwidth is infinite The network is secure Topology never changes The network is homogeneous Consistent resource usage with no spikes All shared resources are available from all places With supercharging our microservice architectures by following devops and gitops principles, we can rollout as fast as possible.</description></item><item><title>Litmus</title><link>/doc/kubernetes/chaos-engineering/litmus/litmus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/chaos-engineering/litmus/litmus/</guid><description>Install Litmus namespace based Installation Guide
Create scc:
allowHostDirVolumePlugin: true allowHostIPC: false allowHostNetwork: false allowHostPID: true allowHostPorts: false allowPrivilegeEscalation: true allowPrivilegedContainer: true allowedCapabilities: null apiVersion: security.openshift.io/v1 defaultAddCapabilities: - NET_ADMIN - SYS_ADMIN fsGroup: type: MustRunAs kind: SecurityContextConstraints metadata: name: litmus priority: null readOnlyRootFilesystem: false requiredDropCapabilities: - KILL - MKNOD - SETUID - SETGID runAsUser: type: RunAsAny seLinuxContext: type: MustRunAs supplementalGroups: type: RunAsAny users: [] volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Bind Service accounts to SCC:</description></item><item><title/><link>/doc/argocd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/argocd/</guid><description> ArgoCD Create application argocd apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: rhe-serverless namespace: argocd spec: destination: namespace: ${NAMESPACE} server: # Clusterroute or Clustername e.g.: https://${CLUSTER_API} project: project_name source: path: src/main/openshift repoURL: https://${TOKEN_NAME}:${TOKEN_VALUE}@gitlab.puzzle.ch/some-repo.git - CreateNamespace=true
Ignore certain properties When using ArgoCD with HPA or Autoscaled Resources (keda.sh), you can ignore certain fields like this:
apiVersion: argoproj.io/v1alpha1 kind: Application spec: # [...] ignoreDifferences: - group: &amp;#34;apps&amp;#34; kind: &amp;#34;Deployment&amp;#34; jsonPointers: - /spec/replicas syncPolicy: syncOptions: - RespectIgnoreDifferences=true</description></item><item><title/><link>/doc/eda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/eda/</guid><description>event driven architecture https://www.youtube.com/watch?v=uxPFoImWpBg&amp;t=7200s</description></item><item><title/><link>/doc/kubernetes/chaos-engineering/kraken/kraken/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/chaos-engineering/kraken/kraken/</guid><description/></item><item><title/><link>/doc/kubernetes/chaos-engineering/litmus/blog/litmus-blog/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/chaos-engineering/litmus/blog/litmus-blog/</guid><description>Chaos Engineering with Litmus Cloud native technologies allow us to bring supercharged architectures up and running in no time. Resources get smaller and more lightweight. Adding Infrastructure as Code and GitOps concepts we are enabled to rollout more frequently and more distributed than ever before. But are we confident in our systems? When will we start building more confidence in our system? Acknowledging that our systems should not be fault-less but rather fault-tolerant, will adopt our view enormously.</description></item><item><title/><link>/doc/kubernetes/hacky-windoof/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/hacky-windoof/</guid><description> Windoof CP File from pod to windows in gitbash
oc exec PODNAME -c CONTAINER_NAME -- bash -c &amp;#34;base64 FILE&amp;#34; | base64 -d &amp;gt; localfile</description></item><item><title/><link>/doc/kubernetes/helm/helm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/helm/helm/</guid><description>Helm Post install hook to test CRD readiness apiVersion: batch/v1 kind: Job metadata: name: &amp;#34;{{ .Release.Name }}&amp;#34; labels: app.kubernetes.io/managed-by: {{ .Release.Service | quote }} app.kubernetes.io/instance: {{ .Release.Name | quote }} app.kubernetes.io/version: {{ .Chart.AppVersion }} helm.sh/chart: &amp;#34;{{ .Chart.Name }}-{{ .Chart.Version }}&amp;#34; annotations: # This is what defines this resource as a hook. Without this line, the # job is considered part of the release. &amp;#34;helm.sh/hook&amp;#34;: post-install &amp;#34;helm.sh/hook-weight&amp;#34;: &amp;#34;-5&amp;#34; &amp;#34;helm.sh/hook-delete-policy&amp;#34;: hook-succeeded spec: template: metadata: name: &amp;#34;{{ .</description></item><item><title/><link>/doc/kubernetes/istio/servicemesh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/istio/servicemesh/</guid><description>Service Mesh Operator (Istio) Installing Operator Install ElasticSearch Operator Install Jaeger Operator Install Service Mesh Operator Install Kiali Operator Setup the control plane UI Method:
Create new ServiceMeshControlPlane:
Create namespace (e.g. istio-system) Openshift Web Console Installed Operators Openshift Service Mesh Istio ServiceMeshControlPlane Create new Control Plane Declarative Method:
Create file for ServiceMeshControlPlane (smcp):
apiVersion: maistra.io/v2 kind: ServiceMeshControlPlane metadata: finalizers: - maistra.io/istio-operator generation: 1 name: basic namespace: istio-system spec: grafana: enabled: true jaeger: install: storage: type: Memory kiali: enabled: true prometheus: enabled: true policy: type: Istiod profiles: - default telemetry: type: Istiod tracing: sampling: 10000 type: Jaeger version: v2.</description></item><item><title/><link>/doc/kubernetes/openshift/amq/amq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/openshift/amq/amq/</guid><description>AMQ Openshift Setup Links Blog Red Hat
Install Operator
Check and install resources.
Configuration Almost all aspects of the broker on openshift are configured via the CR available. Most settings can be applied in the main resource ActiveMQArtemis.
Addresses Adresses can be configured with the ActiveMQArtemisAddress custom resource. An example can be found here.
Broker The broker will be configured with the ActiveMQArtemis custom resouce. Example.
Securit** Users, Groups and their roles and access strategies to addresses / queues can be defined in the ActiveMQArtemisSecurity custom resource.</description></item><item><title/><link>/doc/kubernetes/openshift/amq/consumer/README/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/openshift/amq/consumer/README/</guid><description>consumer Project This project uses Quarkus, the Supersonic Subatomic Java Framework.
If you want to learn more about Quarkus, please visit its website: https://quarkus.io/ .
Running the application in dev mode You can run your application in dev mode that enables live coding using:
./mvnw compile quarkus:dev NOTE: Quarkus now ships with a Dev UI, which is available in dev mode only at http://localhost:8080/q/dev/.
Packaging and running the application The application can be packaged using:</description></item><item><title/><link>/doc/kubernetes/openshift/amq/producer/README/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/openshift/amq/producer/README/</guid><description>producer Project This project uses Quarkus, the Supersonic Subatomic Java Framework.
If you want to learn more about Quarkus, please visit its website: https://quarkus.io/ .
Running the application in dev mode You can run your application in dev mode that enables live coding using:
./mvnw compile quarkus:dev NOTE: Quarkus now ships with a Dev UI, which is available in dev mode only at http://localhost:8080/q/dev/.
Packaging and running the application The application can be packaged using:</description></item><item><title/><link>/doc/kubernetes/openshift/crc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/openshift/crc/</guid><description> Code Ready Containers Delete existing cluster
# $ crc delete Setup and start cluster
# $ crc setup $ crc start</description></item><item><title/><link>/doc/kubernetes/openshift/openshift/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/openshift/openshift/</guid><description>OpenShift Go Templates Get route host
curl $(oc get route ROUTE_NAME -o go-template=&amp;#39;{{(index .status.ingress 0).host}}&amp;#39;)/data Compare ready and desired pod count
[[ $(oc -n NAMESPACE get dc DC_NAME -o go-template=&amp;#39;{{.status.readyReplicas}}&amp;#39;) == $(oc -n NAMESPACE get dc DC_NAME -o go-template=&amp;#39;{{.status.replicas}}&amp;#39;) ]] Compare environment variables of pods
[[ $(oc get pod data-consumer-75f69c845d-564bq -o go-template=&amp;#39;{{(index (index .spec.containers 0).env 1)}}&amp;#39;) == $(oc get pod data-producer-74f5d89975-vwxhw -o go-template=&amp;#39;{{(index (index .spec.containers 0).env 0)}}&amp;#39;) ]] Simple automated apply environment script #!</description></item><item><title/><link>/doc/kubernetes/prometheus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/kubernetes/prometheus/</guid><description>Prometheus Get CPU and Memory usage of kubernetes pods Get effecitve cpu usage of pod rate(conatiner_cpu_usage_seconds_total{container_name != &amp;#34;&amp;#34;, namespace = &amp;#34;&amp;#34;}[5m]) Cpu requests kube_pod_container_resource_requests_cpu_cores{namespace = &amp;#34;&amp;#34;} Effective memory usage sum_by(pod_name, instance)(container_memory_working_set_bytes{namespace=&amp;#34;&amp;#34;} / 1024^3) Memory request kube_pod_container_resource_requests_memory_bytes{namespace = &amp;#34;&amp;#34;} Memory usage relative to request sum by (container, pod, namespace) (container_memory_working_set_bytes{namespace = &amp;#34;pitc-rhe-serverless&amp;#34;,container!=&amp;#34;POD&amp;#34;,container!=&amp;#34;&amp;#34;}) / sum by (container, pod, namespace) (kube_pod_container_resource_requests_memory_bytes{namespace = &amp;#34;pitc-rhe-serverless&amp;#34;}) Prometheus Rules Create alerting rules in Prometheus:
--- apiVersion: monitoring.</description></item><item><title/><link>/doc/linux/bash/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/doc/linux/bash/</guid><description>Bash Port checking Check what&amp;rsquo;s running on port 8083.
netstat -ltnp | grep -w ':8083'
Read pid startup command cat /proc/{PID}/cmdline</description></item></channel></rss>